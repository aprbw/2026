<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Exploring Visual Inversion Problems: From World Models to Remote Sensing Image-to-Image Translation | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="This blog post explores the underexplored problem of visual inversion in the context of world models, arguing that remote sensing data provides optimal native image pairs for image-to-image translation tasks, and investigates whether current vision foundation models can solve these visual inversion problems through in-context learning."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/remote-sensing-foundation-models-to-world-models/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Exploring Visual Inversion Problems: From World Models to Remote Sensing Image-to-Image Translation",
            "description": "This blog post explores the underexplored problem of visual inversion in the context of world models, arguing that remote sensing data provides optimal native image pairs for image-to-image translation tasks, and investigates whether current vision foundation models can solve these visual inversion problems through in-context learning.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Exploring Visual Inversion Problems: From World Models to Remote Sensing Image-to-Image Translation</h1> <p>This blog post explores the underexplored problem of visual inversion in the context of world models, arguing that remote sensing data provides optimal native image pairs for image-to-image translation tasks, and investigates whether current vision foundation models can solve these visual inversion problems through in-context learning.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#world-models-as-maps-the-foundation-of-inversion">World Models as Maps: The Foundation of Inversion</a> </div> <div> <a href="#inversion-problems-from-language-to-vision">Inversion Problems: From Language to Vision</a> </div> <div> <a href="#image-to-image-translation-a-visual-inversion-problem">Image-to-Image Translation: A Visual Inversion Problem</a> </div> <div> <a href="#the-limitations-of-man-made-feature-maps">The Limitations of Man-Made Feature Maps</a> </div> <div> <a href="#remote-sensing-native-image-pairs-for-optimal-translation">Remote Sensing: Native Image Pairs for Optimal Translation</a> </div> <div> <a href="#exploring-visual-inversion-with-foundation-models">Exploring Visual Inversion with Foundation Models</a> </div> </nav> </d-contents> <h2 id="world-models-as-maps-the-foundation-of-inversion">World Models as Maps: The Foundation of Inversion</h2> <div style="border: 2px solid #ff9800; background-color: #fff3e0; padding: 15px; border-radius: 5px; margin: 15px 0;"> <strong>Takeaway:</strong> World models can be understood as maps or functions in a state space. Given a mapping from input X to output Y, a true world model should be able to perform both forward prediction (X → Y) and inverse inference (Y → X) for any data pair that satisfies the mapping correlation. </div> <p>Recent work, particularly a comprehensive survey <d-cite key="dingUnderstandingWorldPredicting2025"></d-cite> and related research <d-cite key="vafaWhatHasFoundation2025,vafaEvaluatingWorldModel2024,zhangWhenDoNeural2025"></d-cite>, provides a compelling definition of world models that frames them as <strong>maps of functions</strong> in a state space. In this framework, we have a state space where we map input X to output Y. The key insight is that for any given data pair that satisfies this mapping correlation, we should always be able to derive the result Y given the input X, or conversely, perform the inversion to derive the input X given the output Y.</p> <p>This bidirectional capability—the ability to both predict forward and invert backward—is fundamental to what constitutes a true world model <d-cite key="kong3D4DWorldModeling2025"></d-cite>. It’s not merely about learning a one-way transformation, but about understanding the underlying mapping structure well enough to traverse it in both directions.</p> <h2 id="inversion-problems-from-language-to-vision">Inversion Problems: From Language to Vision</h2> <div style="border: 2px solid #ff9800; background-color: #fff3e0; padding: 15px; border-radius: 5px; margin: 15px 0;"> <strong>Takeaway:</strong> Large language models have demonstrated remarkable capability in solving inversion problems in the semantic domain. However, this type of inversion problem remains underexplored in the vision field, presenting a significant research opportunity. </div> <p>A classic example of inversion in the language domain illustrates this concept beautifully. If we know that “A is B’s father,” we can derive through inversion that “B is A’s son.” This demonstrates how language models can understand and manipulate semantic relationships bidirectionally <d-cite key="liHowDoLanguage2025,wangCanLanguageModels2024"></d-cite>.</p> <p>The existing literature has extensively explored large language models’ capability to solve this kind of world model problem in the semantic domain <d-cite key="mancoridisPotemkinUnderstandingLarge2025"></d-cite>. However, <strong>this type of inversion problem is still underexplored in the vision field</strong>. While language models excel at semantic inversion, the visual domain presents unique challenges and opportunities that have not been fully addressed.</p> <h2 id="image-to-image-translation-a-visual-inversion-problem">Image-to-Image Translation: A Visual Inversion Problem</h2> <div style="border: 2px solid #ff9800; background-color: #fff3e0; padding: 15px; border-radius: 5px; margin: 15px 0;"> <strong>Takeaway:</strong> Image-to-image translation represents a form of visual inversion problem. Tasks like translating from sketch images to real images (and vice versa) or from age maps to real images (and vice versa) have been well-studied for years, but they represent a vision-only problem that does not involve semantic information. </div> <p>When we move from the language field to the vision field, one of the most similar cases is <strong>image-to-image translation</strong>. This encompasses tasks such as:</p> <ul> <li>Sketch image ↔ Real image</li> <li>Age map ↔ Real image</li> <li>Segmentation map ↔ Real image</li> </ul> <p>This image-to-image translation paradigm has been well-studied for years <d-cite key="isolaImageToImageTranslation2017,zhuUnpairedImageToImage2017,zhuUnpairedImagetoimageTranslation2020"></d-cite> and represents a kind of inversion problem. Recent advances have explored bidirectional translation using diffusion models <d-cite key="liBBDMImagetoimageTranslation2023,xueBiBBDMBidirectionalImage2025,zhouDenoisingDiffusionBridge2024,chungDirectDiffusionBridge2023,liuI2SBImagetoImageSchrodinger2023"></d-cite> and other generative approaches <d-cite key="leeMaskGANTowardsDiverse2020"></d-cite>. Notably, this is a <strong>vision-only problem</strong> that does not involve any semantic information—it’s purely about learning the visual transformation between different representations of the same underlying scene or object.</p> <h2 id="the-limitations-of-man-made-feature-maps">The Limitations of Man-Made Feature Maps</h2> <div style="border: 2px solid #ff9800; background-color: #fff3e0; padding: 15px; border-radius: 5px; margin: 15px 0;"> <strong>Takeaway:</strong> Existing image-to-image translation datasets generally use man-made intermediate representations (sketches, segmentation maps, age maps) that are algorithmically defined. These features are implicitly derived from the original image, making it hard to evaluate accuracy and control the quality of extracted features. </div> <p>However, one fundamental fallback of this problem definition is that <strong>existing image-to-image translation datasets are generally man-made</strong>. In these datasets, we typically have a real-world image either as the source image or the target image, and the corresponding translated image is something like:</p> <ul> <li>Age maps</li> <li>Segmentation maps</li> <li>Sketch maps</li> </ul> <p>These are generally <strong>human-made or defined by some sort of algorithms</strong>. The problem is that these features are actually implicitly derived from the original image, and the details, accuracy, and degree of these extracted features can vary significantly. It’s hard to evaluate and control the quality of these intermediate representations.</p> <p>Moreover, nowadays strong general models can achieve impressive image-to-image translation results, but <strong>existing metrics are field-specific metrics</strong> that capture the difference or visual appearance of the results. It’s hard to distinguish between different models even though they have different metric performance—they may look a bit different or in different styles, but it’s hard to say that one is definitively better than another in appearance.</p> <p><strong>The underlying problem is that these man-made feature maps as the source or target results are not optimal</strong> because we do not introduce new data. The intermediate representations are essentially lossy compressions of the original image, and we’re learning to translate between these compressed representations rather than between truly complementary views of the same scene.</p> <h2 id="remote-sensing-native-image-pairs-for-optimal-translation">Remote Sensing: Native Image Pairs for Optimal Translation</h2> <div style="border: 2px solid #ff9800; background-color: #fff3e0; padding: 15px; border-radius: 5px; margin: 15px 0;"> <strong>Takeaway:</strong> Remote sensing data provides native image pairs from different sensor modalities (optical, SAR, multispectral, hyperspectral) that capture the same geolocation at the same time. These complementary modalities represent optimal image-to-image translation pairs that reflect real-world applications. </div> <p>A novel solution to this problem lies in the <strong>remote sensing domain</strong>. For Earth observation, we have different sorts of sensors that capture different kinds of modalities of satellite images. Common sources include:</p> <ul> <li> <strong>Landsat 7/8</strong> (optical imagery)</li> <li> <strong>Sentinel-1/2</strong> (SAR and multispectral)</li> <li>High-resolution commercial satellite imagery</li> </ul> <p>These satellite images have a great number of different spectra—far more than the traditional RGB channels in general computer vision domains. For a certain geolocation at a certain time, the captured images could have multiple instances:</p> <ul> <li><strong>RGB optical images</strong></li> <li><strong>SAR (Synthetic Aperture Radar) images</strong></li> <li><strong>Multispectral images</strong></li> <li><strong>Hyperspectral images</strong></li> </ul> <p>This natively gives different kinds of attributes that reflect the same object. <strong>This is what makes optimal image-to-image translation pairs</strong>—they reflect real-world applications where different modalities are complementary to each other.</p> <p>This is quite a hard task because different channels, different bands, and different spectra represent different information from the same object. Intuitively, there is no easy way we can yield the optimal translation results from one modality to another, which makes it an ideal testbed for visual inversion problems.</p> <p><strong>Remote sensing data compared to general computer vision data is much more suitable for the image-to-image translation task</strong>, and the existing literature already has a great number of resources. Notable datasets include:</p> <ul> <li> <strong>OpenEarthMap-SAR</strong> <d-cite key="xiaOpenEarthMapSARBenchmark2025"></d-cite> </li> <li> <strong>SARLANG-1M</strong> <d-cite key="weiSARLANG1MBenchmark2025"></d-cite> </li> <li> <strong>EarthView</strong> <d-cite key="velazquezEarthViewLargeScale2025"></d-cite> </li> <li> <strong>TerraMesh</strong> <d-cite key="blumenstielTerraMeshPlanetaryMosaic2025"></d-cite> </li> <li> <strong>SAR-TEXT</strong> <d-cite key="heSARTEXTLargeScale2025"></d-cite> </li> <li> <strong>MMEarth</strong> <d-cite key="nedungadiMMEarthExploringMultimodal2024"></d-cite> </li> </ul> <p>These papers provide geolocation-aligned remote sensing data across different modalities. The data sources come from Sentinel-1, Sentinel-2, high-resolution images from commercial satellites, and other feature maps like NDVI (Normalized Difference Vegetation Index), DEM (Digital Elevation Model), depth maps, and so on. Recent work has also explored remote sensing-oriented world models <d-cite key="luRemoteSensingOriented2025"></d-cite> and foundation models for Earth observation <d-cite key="wangTowardsUnifiedCopernicus2025,tsengGalileoLearningGlobal2025,szwarcmanPrithviEO20Versatile2025,danishTerraFMScalableFoundation2025"></d-cite>.</p> <h2 id="exploring-visual-inversion-with-foundation-models">Exploring Visual Inversion with Foundation Models</h2> <div style="border: 2px solid #ff9800; background-color: #fff3e0; padding: 15px; border-radius: 5px; margin: 15px 0;"> <strong>Takeaway:</strong> We formulate the visual inversion problem as a zero-shot or one-shot in-context learning task, where vision foundation models are given example pairs and asked to perform the inverse transformation without explicit specification of the transformation rules, allowing the model to explore the visual transformation through visual cues alone. </div> <p>Given the success we’ve seen in LLMs that are able to solve inversion problems and have pathways to world models, but the underexplored nature of this problem in the vision field, we develop such an inversion problem using our remote sensing image-to-image translation dataset.</p> <p><strong>What we are going to explore is a task setup</strong> where:</p> <ul> <li> <strong>Input</strong>: We have an image-to-image translation dataset</li> <li> <strong>Source data</strong>: Optimal optical images (like RGB images)</li> <li> <strong>Target data</strong>: SAR images</li> </ul> <p>We use existing vision foundation models, especially the <strong>unified understanding and generation multimodal models</strong> like:</p> <ul> <li> <strong>OmniGen</strong> <d-cite key="xiaoOmniGenUnifiedImage2025"></d-cite> for unified image generation</li> <li> <strong>UniReal</strong> <d-cite key="chenUniRealUniversalImage2025"></d-cite> for universal image generation and editing via learning real-world dynamics</li> <li> <strong>ChronoEdit</strong> <d-cite key="wuChronoEditTowardsTemporal2025"></d-cite> for temporal reasoning in image editing and world simulation</li> <li>Other image editing models that leverage video generation for image manipulation <d-cite key="rotsteinPathwaysImageManifold2025"></d-cite> and models that are able to accept reference images as input to do in-context generation</li> </ul> <p>We also explore models based on <strong>joint-embedding predictive architectures</strong> <d-cite key="assranSelfsupervisedLearningImages2023,chenDenoisingJointEmbedding2025,moConnectingJointEmbedding2024"></d-cite> and video prediction models <d-cite key="assranVJEPA2SelfSupervised2025,bardesRevisitingFeaturePrediction2024"></d-cite> that have shown promise in understanding visual transformations. Additionally, we consider unified multimodal pretraining approaches <d-cite key="dengEmergingPropertiesUnified2025,radfordLearningTransferableVisual2021"></d-cite> that bridge understanding and generation capabilities.</p> <p><strong>What we are going to do</strong> is formulate this inversion problem in a <strong>zero-shot or one-shot in-context learning style</strong>. For example:</p> <ul> <li>We have source image (optical image A) and optical image B</li> <li>We have corresponding target images (SAR image A and SAR image B)</li> <li>We put optical image A and SAR image A along with SAR image B as the reference images for the model</li> <li>We give a prompt like: “We denote the first image (optical image A) as the input image and the result image is SAR image A. Now we give you the result image (SAR image B), please generate the input image.”</li> </ul> <p>We formulate this inversion problem <strong>in-context</strong>, and it is noted that in the prompt, we do not explicitly specify how the source and target images are related or what the transformation means. <strong>We allow the model to explore the transformation between the given visual inputs through the visual cues themselves</strong>.</p> <p>What we are going to explore is whether current vision foundation or multimodal foundation models have the capability to do this kind of <strong>visual inverse problem</strong>—can they understand the mapping between optical and SAR imagery well enough to perform the inverse transformation when given only visual examples, without explicit semantic or textual guidance?</p> <p>This represents a fundamental test of whether these models have developed true world model capabilities in the visual domain <d-cite key="kangHowFarVideo2025,chiEmpoweringWorldModels2025,wuVideoWorldModels2025,chenLearningWorldModels2025"></d-cite>, or whether they are still limited to pattern matching and forward prediction without the ability to reason backward through learned transformations. Recent work on video world models <d-cite key="liuWorldWeaverGeneratingLongHorizon2025,gillmanForcePromptingVideo2025,caoDimensionReductionAttack2025"></d-cite> and 3D/4D world modeling <d-cite key="guiImageWorldGenerating2025,zhouLearning3DPersistent2025,trevithickSimVSSimulatingWorld2025"></d-cite> suggests that models are beginning to capture more sophisticated understanding of visual transformations, but the specific problem of visual inversion in the context of remote sensing modalities remains largely unexplored.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-remote-sensing-foundation-models-to-world-models.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fans/">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-long-context/">Text-to-Image compression for long context understanding</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/using-large-language-models-to-simulate-and-predict-human-decision-making/">Using Large Language Models to Simulate and Predict Human Decision-Making</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/useful-calibrated-uncertainties/">What (and What Not) are Calibrated Uncertainties Actually Useful for?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/unlearning-or-untraining/">Is your algorithm Unlearning or Untraining?</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>