<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://iclr-blogposts.github.io/2026/feed.xml" rel="self" type="application/atom+xml"/><link href="https://iclr-blogposts.github.io/2026/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-11T00:14:40+00:00</updated><id>https://iclr-blogposts.github.io/2026/feed.xml</id><title type="html">ICLR Blogposts 2026</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">From U-Nets to DiTs: The Architectural Evolution of Text-to-Image Diffusion Models (2021–2025)</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/diffusion-architecture-evolution/" rel="alternate" type="text/html" title="From U-Nets to DiTs: The Architectural Evolution of Text-to-Image Diffusion Models (2021–2025)"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/diffusion-architecture-evolution</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/diffusion-architecture-evolution/"><![CDATA[<style>[data-theme="dark"] figcaption.caption{color:white!important}[data-theme="dark"] .key-differences{color:black!important}[data-theme="dark"] .key-differences strong{color:black!important}[data-theme="dark"] .key-differences li{color:black!important}[data-theme="light"] figcaption.caption{color:black!important}[data-theme="light"] .key-differences{color:black!important}[data-theme="light"] .key-differences strong{color:black!important}[data-theme="light"] .key-differences li{color:black!important}[data-theme="light"] .themed-image{content:url("/2026/assets/img/2026-04-27-diffusion-architecture-evolution/teaser_white.png")}[data-theme="dark"] .themed-image{content:url("/2026/assets/img/2026-04-27-diffusion-architecture-evolution/teaser_black.png")}.key-differences{border:2px solid #ff9800;background-color:#fff3e0;padding:15px;border-radius:5px;margin:15px 0}.key-differences ul{margin-top:10px;margin-bottom:0}</style> <div class="l-page"> <figure class="themed-figure"> <img class="themed-image" alt="A hero image summarizing the evolution of diffusion model architectures from U-Nets to Transformers." src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/teaser_white.png"/> <figcaption class="caption">Diffusion Image Model Architecture Evolution.</figcaption> </figure> </div> <h2 id="tldr">TL;DR</h2> <p>As diffusion systems scale, the biggest wins tend to come from leveraging compute with broad, general methods rather than hand-crafting ever more specific tricks <d-cite key="richsuttonBitterLesson2019"></d-cite>. At the same time, we should keep sight of the “hardware lottery”: what succeeds can reflect today’s accelerators and tooling as much as inherent merit <d-cite key="hookerHardwareLottery2021"></d-cite>.</p> <h2 id="preliminaries-diffusion-models-for-image-generation">Preliminaries: Diffusion Models for Image Generation</h2> <p>Diffusion models have emerged as a powerful paradigm for generative modeling by learning to reverse a gradual noise corruption process. The fundamental approach involves two key stages: a <strong>forward diffusion process</strong> that systematically adds noise to data until it becomes pure Gaussian noise, and a <strong>reverse denoising process</strong> where a neural network gradually removes this noise to generate new samples.</p> <p>This framework has demonstrated remarkable success across diverse domains including image generation, audio synthesis, video generation, and even applications in natural language processing and molecular design. The generality of the diffusion framework makes it particularly attractive for complex generative tasks.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lilian_DDPM-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lilian_DDPM-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lilian_DDPM-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lilian_DDPM.png" width="100%" height="auto" alt="Diagram showing the forward noising process and the reverse denoising process in diffusion models." loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">The Markov chain for the forward and reverse diffusion processes, which generate a sample by slowly adding (and removing) noise. Image Credit: <d-cite key="wengWhatAreDiffusion2021"></d-cite></figcaption> </figure> <p>For readers seeking a comprehensive introduction to diffusion model fundamentals, we recommend Yang Song’s excellent exposition on <a href="https://yang-song.net/blog/2021/score/">score-based generative modeling</a> <d-cite key="song2019generative"></d-cite> and Lilian Weng’s detailed overview of <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">diffusion models</a> <d-cite key="wengWhatAreDiffusion2021"></d-cite>.</p> <h2 id="interactive-timeline">Interactive Timeline</h2> <div class="l-page"> <iframe src="/2026/assets/html/2026-04-27-diffusion-architecture-evolution/timeline.html" frameborder="0" scrolling="yes" height="700px" width="100%"></iframe> </div> <h2 id="the-u-net-era">The U-Net Era</h2> <p>The early pioneering works in diffusion-based image generation predominantly adopted <strong>U-Net architectures</strong> <d-cite key="ronnebergerUNetConvolutionalNetworks2015"></d-cite> as their neural network backbone. This choice was largely influenced by U-Net’s proven success in various computer vision tasks <d-cite key="linRefineNetMultiPathRefinement2017"></d-cite><d-cite key="salimansPixelCNNImprovingPixelCNN2017"></d-cite>.</p> <p>The foundational models in this era established the core principles of diffusion-based generation. <strong>NCSN</strong> (Noise Conditional Score Network) <d-cite key="song2019generative"></d-cite> pioneered score-based generative modeling using a RefineNet backbone <d-cite key="linRefineNetMultiPathRefinement2017"></d-cite>, while <strong>DDPM</strong> (Denoising Diffusion Probabilistic Models) <d-cite key="hoDenoisingDiffusionProbabilistic2020"></d-cite> established the probabilistic framework using a PixelCNN++ architecture <d-cite key="salimansPixelCNNImprovingPixelCNN2017"></d-cite>. Subsequent refinements including <strong>NCSNv2</strong> <d-cite key="songImprovedTechniquesTraining2020"></d-cite>, <strong>IDDPM</strong> <d-cite key="nicholImprovedDenoisingDiffusion2021"></d-cite>, <strong>ADM</strong> (Ablated Diffusion Model) <d-cite key="dhariwalDiffusionModelsBeat2021"></d-cite>, and <strong>SDE</strong> (Score-based Diffusion via Stochastic Differential Equations) <d-cite key="songScoreBasedGenerativeModeling2021"></d-cite> built upon these foundations with architectural variations similar to DDPM or NCSN. However, these early models focused primarily on unconditional image generation and lacked text-to-image capabilities.</p> <p>The breakthrough for text-to-image generation came with <strong>LDM</strong> (Latent Diffusion Models, also known as Stable Diffusion) <d-cite key="rombachHighResolutionImageSynthesis2022"></d-cite>, which introduced a latent U-Net architecture to enable efficient text-conditioned generation. Following this success, several notable U-Net-based text-to-image models emerged, each exploring different architectural innovations within the U-Net paradigm:</p> <table> <thead> <tr> <th>Model</th> <th>Gen. (#Param)</th> <th>Txt. (#Param)</th> <th>Total (#Param)</th> <th>Release Date</th> </tr> </thead> <tbody> <tr> <td>SD v2.1 <d-cite key="rombachHighResolutionImageSynthesis2022"></d-cite></td> <td>0.87B</td> <td>0.34B</td> <td>1.29B</td> <td>2022-12-07</td> </tr> <tr> <td>Kandinsky <d-cite key="razzhigaevKandinskyImprovedTexttoImage2023"></d-cite></td> <td>1.23B</td> <td>0.56B</td> <td>1.86B</td> <td>2023-01-01</td> </tr> <tr> <td>UniDiffuser <d-cite key="baoOneTransformerFits2023"></d-cite></td> <td>0.95B</td> <td>0.12B</td> <td>1.25B</td> <td>2023-05-12</td> </tr> <tr> <td>SDXL <d-cite key="podellSDXLImprovingLatent2024"></d-cite></td> <td>2.57B</td> <td>0.82B</td> <td>3.47B</td> <td>2023-06-25</td> </tr> <tr> <td>Kandinsky 3 <d-cite key="arkhipkinKandinsky30Technical2024"></d-cite><d-cite key="arkhipkinKandinsky3TexttoImage2024"></d-cite></td> <td>3.06B</td> <td>8.72B</td> <td>12.05B</td> <td>2023-12-11</td> </tr> <tr> <td>Stable Cascade (Würstchen) <d-cite key="perniasWurstchenEfficientArchitecture2024"></d-cite></td> <td>1.56B</td> <td>0.69B</td> <td>2.28B</td> <td>2024-02-07</td> </tr> </tbody> </table> <p>The standard U-Net architecture for diffusion models typically consists of an <strong>encoder</strong> that progressively downsamples the noisy input, a <strong>bottleneck</strong> middle block that processes compressed representations, and a <strong>decoder</strong> that upsamples back to the original resolution. Crucially, <strong>skip connections</strong> preserve fine-grained spatial information across corresponding encoder and decoder stages.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/diffusion_unet_illustration-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/diffusion_unet_illustration-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/diffusion_unet_illustration-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/diffusion_unet_illustration.png" width="100%" height="auto" alt="U-Net backbone used in diffusion models with time conditioning injected into residual blocks and skip connections between encoder and decoder." loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">A typical U-Net backbone used in diffusion models with time conditioning. Time representation uses sinusoidal positional embeddings or random Fourier features; these time features are injected into residual blocks via simple spatial addition or adaptive group normalization layers. Image Credit: <d-cite key="CVPR2023Tutorial"></d-cite>.</figcaption> </figure> <h2 id="the-dits-era">The DiTs Era</h2> <p>As U-Net–based models began to hit a scaling ceiling (e.g., SDXL with ~2.6B parameters <d-cite key="podellSDXLImprovingLatent2024"></d-cite>), naive scaling proved ineffective, motivating a shift towards alternative backbones. The introduction of Diffusion Transformers (DiTs) <d-cite key="Peebles_2023_ICCV"></d-cite> marks a significant paradigm shift by recasting image generation as a patch-sequence modeling problem solved with transformer blocks. This approach offers several key advantages over U-Nets, including superior <strong>scalability</strong> via stacked DiT blocks, the ability to capture <strong>global context</strong> via self-attention for long-range dependencies, and a <strong>unified</strong> architecture that leverages advances in multimodal integration.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/dit-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/dit-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/dit-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/dit.png" width="100%" height="auto" alt="DiT Architecture." loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">The Diffusion Transformer (DiT) architecture. Left: We train conditional latent DiT models. The input latent is decomposed into patches and processed by several DiT blocks. Right: Details of our DiT blocks. We experiment with variants of standard transformer blocks that incorporate conditioning via adaptive layer norm, cross-attention and extra input tokens. Adaptive layer norm works best. Image Credit: <d-cite key="Peebles_2023_ICCV"></d-cite>.</figcaption> </figure> <table> <thead> <tr> <th>Model</th> <th>Gen. (#Param)</th> <th>Txt. (#Param)</th> <th>Total (#Param)</th> <th>Release Date</th> </tr> </thead> <tbody> <tr> <td>PixArt-$\alpha$ <d-cite key="chenPixArtaFastTraining2024"></d-cite></td> <td>0.61B</td> <td>4.76B</td> <td>5.46B</td> <td>2023/10/06</td> </tr> <tr> <td>Lumina-T2I <d-cite key="gaoLuminaT2XScalableFlowbased2025a"></d-cite></td> <td>~4.7B</td> <td>~7B</td> <td>~15B</td> <td>2024/04/01</td> </tr> <tr> <td>PixArt-$\Sigma$ <d-cite key="chenPIXARTSWeaktoStrongTraining2024a"></d-cite></td> <td>0.61B</td> <td>4.76B</td> <td>5.46B</td> <td>2024/04/11</td> </tr> <tr> <td>Lumina-Next-T2I <d-cite key="zhuoLuminaNextMakingLuminaT2X2024a"></d-cite></td> <td>1.75B</td> <td>2.51B</td> <td>4.34B</td> <td>2024/05/12</td> </tr> <tr> <td>Stable Diffusion 3 <d-cite key="esserScalingRectifiedFlow2024"></d-cite></td> <td>2.03B</td> <td>5.58B</td> <td>7.69B</td> <td>2024/06/12</td> </tr> <tr> <td>Flux.1-Dev <d-cite key="blackforestlabsFLUX1"></d-cite></td> <td>11.90B</td> <td>4.88B</td> <td>16.87B</td> <td>2024/08/02</td> </tr> <tr> <td>CogView3-Plus <d-cite key="zhengCogView3FinerFaster2024a"></d-cite></td> <td>2.85B</td> <td>4.76B</td> <td>8.02B</td> <td>2024/10/13</td> </tr> <tr> <td>Hunyuan-DiT <d-cite key="liHunyuanDiTPowerfulMultiResolution2024"></d-cite></td> <td>1.50B</td> <td>2.02B</td> <td>3.61B</td> <td>2024/12/01</td> </tr> <tr> <td>SANA <d-cite key="xieSANAEfficientHighResolution2025"></d-cite></td> <td>0.59B</td> <td>2.61B</td> <td>3.52B</td> <td>2025/01/11</td> </tr> <tr> <td>Lumina-Image 2.0 <d-cite key="qinLuminaImage20Unified2025"></d-cite></td> <td>2.61B</td> <td>2.61B</td> <td>5.31B</td> <td>2025/01/22</td> </tr> <tr> <td>SANA 1.5 <d-cite key="xieSANA15Efficient2025a"></d-cite></td> <td>1.60B</td> <td>2.61B</td> <td>4.53B</td> <td>2025/03/21</td> </tr> <tr> <td>HiDream-I1-Dev <d-cite key="caiHiDreamI1HighEfficientImage2025"></d-cite></td> <td>17.11B</td> <td>5.58B</td> <td>22.77B</td> <td>2025/04/06</td> </tr> <tr> <td>CogView4-6B <d-cite key="zhengCogView3FinerFaster2024a"></d-cite></td> <td>3.50B</td> <td>2.00B</td> <td>6.00B</td> <td>2025/05/03</td> </tr> <tr> <td>Qwen-Image <d-cite key="wuQwenImageTechnicalReport2025"></d-cite></td> <td>20.43B</td> <td>8.29B</td> <td>28.85B</td> <td>2025/08/04</td> </tr> </tbody> </table> <h2 id="latest-advancement-in-u-net-and-dit-architecture-design">Latest Advancement in U-Net and DiT Architecture Design</h2> <p>While the transition from U-Net to DiT architectures represents a major paradigm shift, both architectural families have continued to evolve with innovative refinements. In the U-Net domain, <strong>two-stage cascaded approaches</strong> <d-cite key="hoCascadedDiffusionModels2022"></d-cite><d-cite key="sahariaImageSuperResolution2022"></d-cite> decompose generation into a low-resolution base model and specialized super-resolution upsamplers, improving fidelity while maintaining training tractability. <strong>U-ViT</strong> <d-cite key="Bao_2023_CVPR"></d-cite> bridges U-Net and transformer architectures by replacing CNN residual blocks with Vision Transformer blocks while retaining the characteristic U-shaped structure and skip connections, enabling stronger global context modeling with competitive ImageNet performance.</p> <p>“The DiT family has seen rapid advances across multiple dimensions. <strong>Architecture variants</strong> include <strong>SiT</strong> (Scalable Interpolant Transformer), which replaces diffusion with interpolant-based transport for improved stability, and <strong>LiT</strong> (Linear Diffusion Transformer) <d-cite key="xieSANAEfficientHighResolution2025"></d-cite>, which achieves O(n) complexity through linear attention mechanisms enabling higher-resolution generation. <strong>Training efficiency innovations</strong> such as <strong>MDT/MDTv2</strong> <d-cite key="gaoMaskedDiffusionTransformer2023"></d-cite><d-cite key="gaoMDTv2MaskedDiffusion2024"></d-cite> and <strong>MaskDiT</strong> <d-cite key="zhengFastTrainingDiffusion2024"></d-cite> leverage masked latent modeling to achieve 10× faster learning and competitive performance with only 30% of standard training time, while representation-based approaches <strong>REPA</strong> (REPresentation Alignment) <d-cite key="yuRepresentationAlignmentGeneration2025"></d-cite> and <strong>REG</strong> (Representation Entanglement for Generation) <d-cite key="wuRepresentationEntanglementGeneration2025"></d-cite> incorporate external pretrained visual representations to dramatically accelerate training—REPA achieves 17.5× speedup and FID of 1.42, while REG achieves 63× faster training than baseline SiT by entangling image latents with class tokens during denoising with negligible inference overhead. <strong>Architecture refinements</strong> like <strong>DDT</strong> <d-cite key="wangDDTDecoupledDiffusion2025"></d-cite> decouple semantic encoding from high-frequency decoding for 4× faster convergence. In parallel, <strong>U-DiTs</strong> downsample (and later upsample) tokens in a U-shaped DiT, shortening the effective sequence length to reduce attention cost while preserving fine detail for high-resolution synthesis <d-cite key="tianUDiTsDownsampleTokens2024"></d-cite>. Meanwhile, tokenizer innovations leverage pretrained foundation models: <strong>RAE</strong> <d-cite key="zhengDiffusionTransformersRepresentation2025"></d-cite> replaces standard VAEs with pretrained representation encoders (DINO, SigLIP, MAE) achieving FID of 1.13, and <strong>Aligned Visual Foundation Encoders</strong> <d-cite key="chenAligningVisualFoundation2025"></d-cite> employ a three-stage alignment strategy to transform foundation encoders into semantically rich tokenizers, accelerating convergence (gFID 1.90 in 64 epochs) and outperforming standard VAEs in text-to-image generation.”</p> <h2 id="pre-trained-text-to-image-checkpoints">Pre-trained Text-to-Image Checkpoints</h2> <p>The landscape of pre-trained text-to-image models has evolved dramatically since the introduction of Stable Diffusion. These models serve as powerful foundation models that can be adapted for specialized downstream tasks without architectural modifications, simply by fine-tuning on domain-specific datasets.</p> <h2 id="interactive-architecture-explorer">Interactive Architecture Explorer</h2> <div class="l-body"> <iframe id="architecture-explorer-iframe" src="/2026/assets/html/2026-04-27-diffusion-architecture-evolution/model-architecture-explorer.html" frameborder="0" scrolling="no" height="600px" width="100%" style="border: 1px solid #ddd; border-radius: 4px; min-height: 600px;"></iframe> </div> <script>
  // Listen for resize messages from the iframe
  window.addEventListener('message', function(e) {
    if (e.data && e.data.type === 'resize' && e.data.source === 'architecture-explorer') {
      var iframe = document.getElementById('architecture-explorer-iframe');
      if (iframe) {
        iframe.style.height = e.data.height + 'px';
      }
    }
  });
</script> <h3 id="u-net-family">U-Net Family</h3> <p><strong>Stable Diffusion</strong> <d-cite key="rombachHighResolutionImageSynthesis2022"></d-cite> represents the pioneering work in latent diffusion models, adopting a U-Net architecture that operates in a compressed latent space rather than pixel space. This design choice dramatically reduces computational costs while maintaining high-quality generation capabilities. The model combines two key components: a pre-trained variational autoencoder (VAE) for efficient image compression and decompression, and a diffusion model that performs the denoising process in this latent space.<d-footnote>In the prioring work of LDM in the paper <d-cite key="rombachHighResolutionImageSynthesis2022"></d-cite>, the VAE part is adopted a VQ-GAN style from <d-cite key="esserTamingTransformersHighResolution2021"></d-cite>. When it comes to CompVis Stable Diffusion v1.1-v.1.4 and StabilityAI Stable Diffusion v1.5 and v2.x version, the VAE part is turned to AutoEncoderKL style rather than a VQ style.</d-footnote></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sd-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sd-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sd-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sd.png" width="100%" height="auto" alt="Stable Diffusion 1.x - 2.x architecture." loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Stable Diffusion 1.x - 2.x architecture. Image Credit: <d-cite key="esserTamingTransformersHighResolution2021"></d-cite>.</figcaption> </figure> <p><strong>Stable Diffusion XL (SDXL)</strong> <d-cite key="podellSDXLImprovingLatent2024"></d-cite> marked a significant scaling advancement, adopting a two-stage U-Net architecture and increasing the model size from 0.8 billion to 2.6 billion parameters. SDXL remains one of the largest U-Net-based models for image generation and demonstrates improved efficiency and compatibility across diverse domains and tasks. Despite reaching scaling limits, SDXL continues to serve as a foundation for numerous specialized applications.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sdxl-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sdxl-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sdxl-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sdxl.png" width="100%" height="auto" alt="SDXL Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">SDXL Architecture. Image Credit: <d-cite key="podellSDXLImprovingLatent2024"></d-cite>.</figcaption> </figure> <p><strong>Kandinsky</strong> <d-cite key="razzhigaevKandinskyImprovedTexttoImage2023"></d-cite> represents a significant advancement in the U-Net era, introducing a novel exploration of latent diffusion architecture that combines image prior models with latent diffusion techniques. The model features a modified MoVQ implementation as the image autoencoder component and achieves a FID score of 8.03 on the COCO-30K dataset, marking it as the top open-source performer in terms of measurable image generation quality. <strong>Kandinsky 3</strong> <d-cite key="arkhipkinKandinsky30Technical2024"></d-cite><d-cite key="arkhipkinKandinsky3TexttoImage2024"></d-cite> continues this series with improved text understanding and domain-specific performance, presenting a multifunctional generative framework supporting text-guided inpainting/outpainting, image fusion, and image-to-video generation.</p> <p><strong>Stable Cascade</strong> (based on Würstchen architecture) <d-cite key="perniasWurstchenEfficientArchitecture2024"></d-cite> introduces an efficient architecture for large-scale text-to-image diffusion models, achieving competitive performance with unprecedented cost-effectiveness. The key innovation is a latent diffusion technique that learns extremely compact semantic image representations, reducing computational requirements significantly—training requires only 24,602 A100-GPU hours compared to Stable Diffusion 2.1’s 200,000 GPU hours while maintaining state-of-the-art results.</p> <p><strong>UniDiffuser</strong> <d-cite key="baoOneTransformerFits2023"></d-cite> explores transformer-based diffusion models with a unified framework that fits all distributions relevant to multi-modal data in one model. While primarily focused on transformer architectures, this work demonstrates the potential for unified multi-modal generation within the diffusion framework.</p> <h3 id="pixart-alpha-20231006">Pixart-$\alpha$ (2023/10/06)</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/pixart_cost-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/pixart_cost-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/pixart_cost-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/pixart_cost.png" width="100%" height="auto" alt="Cost Comparison" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Comparisons of CO2 emissions and training cost among T2I generators. PIXART-α achieves an exceptionally low training cost of $28,400. Compared to RAPHAEL <d-cite key="xueRAPHAELTexttoImageGeneration2023"></d-cite>, our CO2 emissions and training costs are merely 1.2% and 0.91%, respectively. Image Credit: <d-cite key="chenPixArtaFastTraining2024"></d-cite>.</figcaption> </figure> <p>PixArt-$\alpha$ is motivated by the rising compute and environmental costs of text-to-image systems, seeking near-commercial quality with a much smaller training budget <d-cite key="chenPixArtaFastTraining2024"></d-cite>. In contrast to SD 1.5/2.1, it adopts a large-language-model text encoder (T5) <d-cite key="raffelExploringLimitsTransfer2020"></d-cite>, making it the first open-source diffusion T2I model to use an LLM-based text encoder while keeping the overall design streamlined.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/pixart_alpha-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/pixart_alpha-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/pixart_alpha-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/pixart_alpha.png" width="100%" height="auto" alt="Pixart-α Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Model architecture of PIXART-α. A cross-attention module is integrated into each block to inject textual conditions. To optimize efficiency, all blocks share the same adaLN-single parameters for time conditions. Image Credit: <d-cite key="chenPixArtaFastTraining2024"></d-cite>.</figcaption> </figure> <p>Architecturally, PixArt-$\alpha$ is a latent Diffusion Transformer (DiT): VAE latents are patchified into a token sequence processed by stacked Transformer blocks; each block applies cross-attention to text tokens, and timestep conditioning is injected via a shared adaLN-single, simplifying parameters and conditioning pathways <d-cite key="chenPixArtaFastTraining2024"></d-cite>.</p> <div class="key-differences"> <strong>Key differences vs SD 1.5/2.1</strong> <ul> <li>Transformer sequence-of-patches backbone (no encoder–decoder or skip connections)</li> <li>Shared adaLN for time and unified per-block cross-attention (vs U-Net residual blocks with per-block time MLP/spatial injections)</li> <li>T5 text encoder (LLM) rather than CLIP/OpenCLIP</li> </ul> </div> <h3 id="lumina-t2i-20240401">Lumina-T2I (2024/04/01)</h3> <p>Lumina-T2I is the first entry in the Lumina series from Shanghai AI Lab, aiming for a simple, scalable framework that supports flexible resolutions while maintaining photorealism. Building on the Sora insight that scaling Diffusion Transformers enables generation across arbitrary aspect ratios and durations yet lacks concrete implementation details, Lumina-T2I adopts flow matching to stabilize and accelerate training <d-cite key="gaoLuminaT2XScalableFlowbased2025a"></d-cite>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_t2x-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_t2x-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_t2x-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_t2x.png" width="100%" height="auto" alt="Lumina-T2I Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Lumina-T2I architecture featuring Flag-DiT backbone. Image Credit: <d-cite key="gaoLuminaT2XScalableFlowbased2025a"></d-cite>.</figcaption> </figure> <p>Architecturally, Lumina-T2I uses a Flow-based Large Diffusion Transformer (Flag-DiT) with zero-initialized attention, RoPE <d-cite key="suRoFormerEnhancedTransformer2024"></d-cite>, and KQ-Norm <d-cite key="henryQueryKeyNormalizationTransformers2020"></d-cite>. Latent features are tokenized and processed by Transformer blocks; learnable placeholders such as the [nextline] token and layerwise relative position injection enable robust resolution extrapolation without retraining for each size.</p> <div class="key-differences"> <strong>Key differences vs PixArt-α</strong> <ul> <li>Robust resolution generalization across 512²–1792²</li> <li>Uses one-dimensional RoPE, [nextline] token, and layerwise relative position injection</li> <li>PixArt-α uses absolute positional embeddings limited to the initial layer, degrading at out-of-distribution scales</li> </ul> </div> <h3 id="lumina-next-t2i-20240512">Lumina-Next-T2I (2024/05/12)</h3> <p>Lumina-Next-T2I <d-cite key="zhuoLuminaNextMakingLuminaT2X2024a"></d-cite> targets the core limitations observed in Lumina-T2X—training instability, slow inference, and resolution extrapolation artifacts—by delivering stronger quality and faster sampling while improving zero-shot multilingual understanding. Unlike prior T2I works that rely on CLIP or T5 encoders <d-cite key="raffelExploringLimitsTransfer2020"></d-cite>, the Lumina series adopts decoder-only LLMs as text encoders: Lumina-T2X uses LLaMA-2 7B <d-cite key="touvronLlama2Open2023"></d-cite>, whereas Lumina-Next employs the lighter Gemma-2B to reduce memory and increase throughput. In practice, Lumina-Next shows clear gains on multilingual prompts (vs. CLIP/T5 setups) and further improves text-image alignment with alternative LLMs like Qwen-1.8B and InternLM-7B.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_next-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_next-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_next-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_next.png" width="100%" height="auto" alt="Lumina-Next-T2I Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Lumina-Next-T2I Next-DiT architecture. Image Credit: <d-cite key="zhuoLuminaNextMakingLuminaT2X2024a"></d-cite>.</figcaption> </figure> <p>Architecturally, Lumina-Next introduces the Next-DiT backbone with 3D RoPE and Frequency- and Time-Aware Scaled RoPE for robust resolution extrapolation <d-cite key="suRoFormerEnhancedTransformer2024"></d-cite>. It adds sandwich normalizations to stabilize training (cf. normalization strategies such as KQ-Norm <d-cite key="henryQueryKeyNormalizationTransformers2020"></d-cite>), a sigmoid time discretization schedule to reduce Flow-ODE sampling steps, and a Context Drop mechanism that merges redundant visual tokens to accelerate inference—all while retaining the flow-based DiT formulation of the Lumina family.</p> <div class="key-differences"> <strong>Key differences vs Lumina-T2I</strong> <ul> <li>Next-DiT with 3D RoPE + frequency/time-aware scaling for stronger resolution extrapolation</li> <li>Sandwich normalizations improve stability; sigmoid time schedule reduces sampling steps</li> <li>Context Drop merges redundant tokens for faster inference throughput</li> <li>Decoder-only LLM text encoders (Gemma-2B by default; Qwen-1.8B/InternLM-7B optional) boost zero-shot multilingual alignment vs CLIP/T5</li> </ul> </div> <h3 id="stable-diffusion-3-20240612">Stable Diffusion 3 (2024/06/12)</h3> <p>Stable Diffusion 3 aims to improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales, demonstrating superior performance compared to established diffusion formulations for high-resolution text-to-image synthesis <d-cite key="esserScalingRectifiedFlow2024"></d-cite>. This work presents the first comprehensive scaling study for text-to-image DiTs, establishing predictable scaling trends and correlating lower validation loss to improved synthesis quality across various metrics and human evaluations.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sd3-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sd3-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sd3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sd3.png" width="100%" height="auto" alt="SD3 Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Stable Diffusion 3 Architecture. Image Credit: <d-cite key="esserScalingRectifiedFlow2024"></d-cite>.</figcaption> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/simplified_architecture_sd35-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/simplified_architecture_sd35-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/simplified_architecture_sd35-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/simplified_architecture_sd35.png" width="100%" height="auto" alt="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Simplied Architecture Illustration of Stable Diffusion 3.5 MM-DiT block. Image Source: Stability AI Blog.</figcaption> </figure> <p>Architecturally, SD3 transitions from DiT’s cross-attention blocks to MMDiT (Multimodal Diffusion Transformer) with double-stream blocks that use separate weights for the two modalities, enabling bidirectional flow of information between image and text tokens for improved text comprehension and typography. Unlike SDXL which relies primarily on CLIP encoders, SD3 incorporates both CLIP (L/14 and OpenCLIP bigG/14) and T5-XXL encoders <d-cite key="raffelExploringLimitsTransfer2020"></d-cite>, concatenating pooled outputs and hidden representations to create comprehensive text conditioning with enhanced understanding capabilities.</p> <div class="key-differences"> <strong>Key differences vs SDXL and PixArt-α</strong> <ul> <li>MMDiT double-stream architecture with separate weights per modality and bidirectional information flow (vs single-stream cross-attention)</li> <li>Integrated rectified flow training with perceptually-biased noise sampling (vs standard diffusion formulation)</li> <li>Combined CLIP + T5-XXL text encoding for enhanced text comprehension and typography</li> <li>First comprehensive scaling study demonstrating predictable trends for text-to-image DiTs</li> </ul> </div> <h3 id="flux1-dev-20240802">Flux.1-Dev (2024/08/02)</h3> <p>Flux.1-Dev, developed by former Stability AI core members, aims to scale beyond previous models and achieve superior image quality with more accurate text-to-image synthesis <d-cite key="blackforestlabsFLUX1"></d-cite>. Representing a significant scaling effort, the model features a massive 12 billion parameter generator combined with a 4.7 billion parameter text encoder, marking substantial growth compared to predecessors and establishing new benchmarks in AI-driven image generation capabilities.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/flux_dit-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/flux_dit-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/flux_dit-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/flux_dit.png" width="100%" height="auto" alt="Flux.1-Dev Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Flux.1-Dev MMDiT architecture. Image Credit: <d-cite key="labsFLUX1KontextFlow2025"></d-cite>.</figcaption> </figure> <p>Architecturally, Flux.1-Dev advances beyond SD3’s MMDiT by implementing a hybrid architecture that combines both single-stream and double-stream Multi-Modal Diffusion Transformers, enhancing the model’s ability to process complex visual-textual relationships. Like SD3, it incorporates T5 text encoding <d-cite key="raffelExploringLimitsTransfer2020"></d-cite> and integrates rectified flow techniques for more stable and efficient training, while conducting a comprehensive scaling study that optimizes performance across the substantially larger parameter space.</p> <div class="key-differences"> <strong>Key differences vs SD3</strong> <ul> <li>Hybrid single-stream + double-stream MMDiT architecture (vs purely double-stream MMDiT)</li> <li>Massive scaling to 12B generator + 4.7B text encoder parameters (vs smaller SD3 variants)</li> <li>Enhanced rectified flow implementation optimized for larger scale training</li> <li>Comprehensive scaling study specifically designed for multi-billion parameter DiTs</li> </ul> </div> <h3 id="cogview3--cogview3-plus-20241013">CogView3 &amp; CogView3-Plus (2024/10/13)</h3> <p><strong>CogView3</strong> <d-cite key="zhengCogView3FinerFaster2024a"></d-cite> introduces a <strong>relay diffusion approach</strong> <d-cite key="tengRelayDiffusionUnifying2024"></d-cite> that generates low-resolution images first, then refines them through super-resolution to achieve 2048×2048 outputs. This multi-stage process reduces computational costs while improving quality—CogView3 outperformed SDXL by 77% in human evaluations while using only one-tenth the inference time. The model employs a text-expansion language model to rewrite user prompts, with a base stage generating 512×512 images followed by relaying super-resolution in the latent space.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/cogview3-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/cogview3-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/cogview3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/cogview3.png" width="100%" height="auto" alt="CogView3 Architecture." loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">(left) The pipeline of CogView3. User prompts are rewritten by a text-expansion language model. The base stage model generates 512 × 512 images, and the second stage subsequently performs relaying super-resolution. (right) Formulation of relaying super-resolution in the latent space. Image Credit: <d-cite key="zhengCogView3FinerFaster2024a"></d-cite>.</figcaption> </figure> <p><strong>CogView3-Plus</strong> upgrades to DiT architecture with Zero-SNR scheduling and joint text-image attention for further efficiency gains. This architectural evolution represents a significant step in the CogView series, transitioning from traditional approaches to transformer-based diffusion models while maintaining the efficiency advantages of the relay diffusion framework.</p> <h3 id="hunyuan-dit-20241201">Hunyuan-DiT (2024/12/01)</h3> <p>Hunyuan-DiT, developed by Tencent’s Hunyuan team, aims to create a powerful multi-resolution diffusion transformer capable of fine-grained understanding of both English and Chinese languages, addressing the need for state-of-the-art Chinese-to-image generation with culturally relevant and multilingual capabilities <d-cite key="liHunyuanDiTPowerfulMultiResolution2024"></d-cite>. The model establishes a comprehensive data pipeline with iterative optimization, employing a Multimodal Large Language Model to refine image captions and enhance alignment between textual descriptions and generated images, particularly for intricate Chinese characters and cultural nuances.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/hunyuandit-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/hunyuandit-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/hunyuandit-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/hunyuandit.png" width="100%" height="auto" alt="Hunyuan-DiT Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Hunyuan-DiT multi-resolution architecture. Image Credit: <d-cite key="liHunyuanDiTPowerfulMultiResolution2024"></d-cite>.</figcaption> </figure> <p>Architecturally, Hunyuan-DiT builds upon PixArt-$\alpha$ by incorporating both single-stream and double-stream Multi-Modal Diffusion Transformer (MM-DiT) blocks similar to SD3, enabling efficient handling of complex image generation tasks across multiple resolutions. The model integrates dual text encoders—CLIP for understanding overall semantic content and T5 <d-cite key="raffelExploringLimitsTransfer2020"></d-cite> for nuanced language comprehension including complex sentence structures—combined with enhanced positional encoding to maintain spatial information across different resolutions, facilitating robust multi-resolution generation capabilities.</p> <div class="key-differences"> <strong>Key differences vs PixArt-α</strong> <ul> <li>Single-stream + double-stream MM-DiT blocks for enhanced multi-modal processing (vs single-stream cross-attention)</li> <li>Dual text encoders (CLIP + T5) for semantic and nuanced language understanding (vs T5 only)</li> <li>Multi-resolution diffusion transformer with enhanced positional encoding for robust resolution handling</li> <li>Multimodal LLM-refined captions with fine-grained bilingual (English + Chinese) understanding</li> </ul> </div> <h3 id="sana-20250111">SANA (2025/01/11)</h3> <p>SANA, developed by NVIDIA, aims to enable efficient high-resolution image synthesis up to 4096×4096 pixels while maintaining deployment feasibility on consumer hardware, generating 1024×1024 images in under a second on a 16GB laptop GPU <d-cite key="xieSANAEfficientHighResolution2025"></d-cite>. The model introduces innovations to reduce computational requirements dramatically: DC-AE (deep compression autoencoder) achieves 32× image compression reducing latent tokens significantly, efficient caption labeling and selection accelerate convergence, and Flow-DPM-Solver reduces sampling steps for faster generation.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sana-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sana-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sana-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sana.png" width="100%" height="auto" alt="SANA Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">SANA Linear DiT architecture for efficient high-resolution generation. Image Credit: <d-cite key="xieSANAEfficientHighResolution2025"></d-cite>.</figcaption> </figure> <p>Architecturally, SANA advances beyond PixArt-$\Sigma$ by replacing traditional self-attention mechanisms with Linear Diffusion Transformer (Linear DiT) blocks, enhancing computational efficiency at high resolutions without compromising quality. The model adopts a decoder-only small language model as the text encoder, employing complex human instructions with in-context learning to improve text-image alignment compared to conventional CLIP or T5 encoders. The compact 0.6B parameter model achieves competitive performance with substantially larger models like Flux-12B while being 20 times smaller and over 100 times faster in throughput.</p> <div class="key-differences"> <strong>Key differences vs PixArt-Σ</strong> <ul> <li>Linear DiT replacing traditional self-attention for O(n) complexity vs O(n²) at high resolutions</li> <li>DC-AE with 32× compression reducing latent tokens and memory requirements dramatically</li> <li>Decoder-only language model as text encoder with in-context learning (vs T5)</li> <li>0.6B parameters achieving competitive quality with 12B models while 100× faster throughput</li> </ul> </div> <h3 id="lumina-image-20-20250122">Lumina-Image 2.0 (2025/01/22)</h3> <p>Lumina-Image 2.0 aims to provide a unified and efficient image generative framework that excels in generating high-quality images with strong text-image alignment across diverse generation and editing tasks <d-cite key="qinLuminaImage20Unified2025"></d-cite>. Building upon the Lumina series’ foundation, the model consolidates multiple generation tasks into a cohesive framework, optimizing performance and efficiency to cater to a wide range of image generation applications while achieving competitive scores across multiple benchmarks including FID and CLIP metrics.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_image2-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_image2-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_image2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_image2.png" width="100%" height="auto" alt="Lumina-Image 2.0 Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Lumina-Image 2.0 Unified Next-DiT architecture. Image Credit: <d-cite key="qinLuminaImage20Unified2025"></d-cite>.</figcaption> </figure> <p>Architecturally, Lumina-Image 2.0 advances beyond Lumina-Next-T2I by introducing a unified Next-DiT architecture that seamlessly integrates text-to-image generation and image editing capabilities within a shared framework. The model maintains the Lumina series’ architectural strengths including 3D RoPE <d-cite key="suRoFormerEnhancedTransformer2024"></d-cite>, frequency-aware scaling, and flow-based formulation, while enhancing the framework to support both generation and editing operations efficiently. This unified approach enables the model to leverage shared representations and training strategies across different image generation modalities.</p> <div class="key-differences"> <strong>Key differences vs Lumina-Next-T2I</strong> <ul> <li>Unified Next-DiT framework seamlessly integrating generation and editing (vs generation-only focus)</li> <li>Enhanced multi-task architecture supporting diverse image generation applications within single model</li> <li>Optimized training paradigm leveraging shared representations across generation modalities</li> <li>Competitive performance across FID and CLIP benchmarks with improved efficiency</li> </ul> </div> <h3 id="sana-15-20250321">SANA 1.5 (2025/03/21)</h3> <p>SANA 1.5 aims to push the boundaries of efficient high-resolution image synthesis established by SANA, offering improved performance and scalability through larger model sizes and advanced inference scaling techniques <d-cite key="xieSANA15Efficient2025a"></d-cite>. The model introduces inference scaling via VISA (a specialized NVILA-2B model) that scores and selects top images from large candidate sets, significantly boosting GenEval performance scores—for instance, improving SANA-1.5-4.8B from 81 to 96. This approach demonstrates that post-generation selection can dramatically enhance quality metrics without architectural changes.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sana1_5-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sana1_5-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sana1_5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sana1_5.png" width="100%" height="auto" alt="SANA 1.5 Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">SANA 1.5 improved Linear DiT architecture. Image Credit: <d-cite key="xieSANA15Efficient2025a"></d-cite>.</figcaption> </figure> <p>Architecturally, SANA 1.5 builds upon the original SANA by incorporating an enhanced DC-AE (deep compression autoencoder) to handle higher resolutions and more complex generation tasks, along with advanced Linear DiT blocks featuring more sophisticated linear attention mechanisms to boost efficiency and quality in high-resolution synthesis. The model scales to 4.8B parameters compared to SANA’s 0.6B, providing a robust solution for generating high-quality images with strong text-image alignment suitable for diverse professional applications requiring both quality and computational efficiency.</p> <div class="key-differences"> <strong>Key differences vs SANA</strong> <ul> <li>Inference scaling with VISA model for candidate selection dramatically improving GenEval scores (81→96)</li> <li>Enhanced DC-AE handling higher resolutions and more complex generation tasks</li> <li>Advanced Linear DiT with more sophisticated linear attention mechanisms</li> <li>Scaled to 4.8B parameters providing improved quality while maintaining efficiency advantages</li> </ul> </div> <h3 id="hidream-i1-dev-20250406">HiDream-I1-Dev (2025/04/06)</h3> <p>HiDream-I1, developed by HiDream.ai, addresses the critical trade-off between quality improvements and computational complexity in image generative foundation models, aiming to achieve state-of-the-art image generation quality within seconds while maintaining high efficiency <d-cite key="caiHiDreamI1HighEfficientImage2025"></d-cite>. With 17 billion parameters, the model introduces a sparse Diffusion Transformer structure that enables efficient inference suitable for professional-grade design needs, supporting 4K ultra-high-definition image generation with advanced text comprehension, multi-style adaptation, and precise detail control while optimizing computational requirements through sparsity.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/hidream-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/hidream-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/hidream-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/hidream.png" width="100%" height="auto" alt="HiDream-I1-Dev Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">HiDream-I1-Dev Sparse DiT architecture. Image Credit: <d-cite key="caiHiDreamI1HighEfficientImage2025"></d-cite>.</figcaption> </figure> <p>Architecturally, HiDream-I1 advances beyond Flux.1-Dev and Qwen-Image by implementing a novel sparse DiT structure where only subsets of transformer blocks are activated for each forward pass, dramatically reducing computational costs while maintaining generation quality. The sparse architecture enables the massive 17B parameter model to achieve practical inference speeds comparable to smaller dense models, with efficient diffusion mechanisms supporting multimodal input and providing fine-grained control over generation. This sparse approach represents a paradigm shift in scaling DiT models, demonstrating that architectural efficiency through sparsity can rival quality of substantially denser models.</p> <div class="key-differences"> <strong>Key differences vs Flux.1-Dev and other large DiTs</strong> <ul> <li>Sparse DiT structure activating only subsets of blocks per forward pass for efficient 17B parameter model</li> <li>4K ultra-high-definition generation support with optimized inference speed despite massive scale</li> <li>Advanced sparse attention mechanisms maintaining quality while dramatically reducing computational costs</li> <li>Multimodal input support and fine-grained control optimized for professional-grade design applications</li> </ul> </div> <h3 id="cogview4-6b-20250503">CogView4-6B (2025/05/03)</h3> <p><strong>CogView4-6B</strong> <d-cite key="zhengCogView3FinerFaster2024a"></d-cite> represents the latest advancement in the CogView series, featuring a sophisticated <strong>CogView4Transformer2DModel</strong> architecture that excels in Chinese text rendering and multilingual image generation. The model demonstrates exceptional performance in text accuracy evaluation, achieving precision of 0.6969, recall of 0.5532, and F1 score of 0.6168 on Chinese text benchmarks.</p> <p>CogView4-6B leverages GLM-based text encoding and advanced transformer blocks with RoPE (Rotary Position Embedding) for enhanced spatial understanding and text-image alignment. This architectural sophistication enables the model to achieve superior text rendering capabilities, particularly for complex Chinese characters and multilingual content, setting new standards for text-to-image generation in non-Latin scripts. Available on <a href="https://huggingface.co/zai-org/CogView4-6B">Hugging Face</a> under Apache 2.0 license.</p> <h3 id="qwen-image-20250804">Qwen-Image (2025/08/04)</h3> <p>Qwen-Image represents a monumental scaling achievement in text-to-image synthesis, establishing a new state-of-the-art with its massive 28.85 billion parameter architecture <d-cite key="wuQwenImageTechnicalReport2025"></d-cite>. Developed by Alibaba’s Qwen team, this flagship model aims to push the boundaries of generation quality, text-image alignment, and multimodal understanding through unprecedented scale. The model excels at generating highly detailed, photorealistic images that accurately reflect complex textual prompts, setting new benchmarks for fidelity and coherence in the field.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/qwen_image-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/qwen_image-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/qwen_image-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/qwen_image.png" width="100%" height="auto" alt="Qwen-Image Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Qwen-Image massively scaled MMDiT architecture. Image Credit: <d-cite key="wuQwenImageTechnicalReport2025"></d-cite>.</figcaption> </figure> <p>Architecturally, Qwen-Image employs a massively scaled Multi-Modal Diffusion Transformer (MMDiT) that builds upon the hybrid single- and double-stream designs seen in models like Flux.1-Dev. The generator model alone comprises over 20 billion parameters, combined with a powerful 8.29 billion parameter text encoder for unparalleled language comprehension. This dual-stream approach allows for sophisticated interaction between text and image modalities, enabling precise control over generated content. The model integrates advanced training techniques, including rectified flow and large-scale data curation, to ensure stable and efficient convergence despite its enormous size.</p> <div class="key-differences"> <strong>Key differences vs HiDream-I1-Dev</strong> <ul> <li>Massive dense scaling to 28.85B parameters (vs HiDream's 17B sparse architecture)</li> <li>Focus on state-of-the-art quality through sheer scale (vs HiDream's focus on efficiency via sparsity)</li> <li>Extremely large 8.29B text encoder for superior text-image alignment</li> <li>Represents the pinnacle of the dense DiT scaling paradigm before potential shifts to new architectures</li> </ul> </div> <h2 id="experiments-and-case-studies">Experiments and Case Studies</h2> <p>To comprehensively evaluate the capabilities of different text-to-image diffusion models, we propose a systematic evaluation framework spanning tasks of varying complexity. This section will present case studies of text-to-image generation visualizations using existing checkpoints, assessing their performance across a spectrum of increasingly challenging tasks.</p> <p><strong>Implementation Details<d-footnote>For commercial model, we use ChatGPT webui GPT-5-Instant with the same prompt for each case study for image generation with a default image size as 1024 × 1024</d-footnote>:</strong></p> <table> <thead> <tr> <th>Parameter</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>Precision</td> <td>bfloat16</td> </tr> <tr> <td>Scheduler</td> <td>default</td> </tr> <tr> <td>Steps</td> <td>50</td> </tr> <tr> <td>Guidance Scale</td> <td>7.5</td> </tr> <tr> <td>Resolution</td> <td>512×512</td> </tr> </tbody> </table> <div class="l-page"> <iframe src="/2026/assets/html/2026-04-27-diffusion-architecture-evolution/case-studies.html" width="100%" height="3000" frameborder="0" style="border: none;"></iframe> </div> <script>
  window.addEventListener('message', function(e) {
    if (e.data.type === 'resize' && e.data.source === 'case-studies') {
      const iframe = document.querySelector('iframe[src*="case-studies.html"]');
      if (iframe) {
        iframe.style.height = e.data.height + 'px';
      }
    }
  });
</script> <div class="key-differences"> <strong>Summary of Results</strong> <ul> <li>There is no strong correlation between image model size and image aesthetics (See case study 4).</li> <li>There is no strong correlation between text model size and prompt following (See case study 5).</li> <li>Large models generally work better but always the case.</li> <li>U-Nets based model perform comparativaly worse than DiTs in the similar model size, for instance, SDXL to SANA, Kandinsky-3 to CogView4.</li> <li>StaleDiffusion 3.x continously trained on higher resolution (e.g., 1024px) tends to generate croped results.</li> <li>Not all models are capable to dealing with multilingual prompt (see case study 2).</li> <li>Commercial model such as GPT-Image model works extremely well in aesthetics, prompt following, counting, text rendering and spatial reasoning.</li> </ul> </div> <h2 id="why-scaling-favors-attention">Why Scaling Favors Attention</h2> <p>As diffusion models scaled in data and compute, the active bottleneck shifted from <strong>local fidelity</strong> to <strong>global semantic alignment</strong>, and the community moved accordingly: from U-Nets that hard-wire translation equivariance via convolution to Diffusion Transformers that <strong>learn</strong> equivariances through self-attention. Let \(\mathcal{C}^{\mathrm{conv}}_{G}\) be the class of <strong>translation-equivariant, finite-support Toeplitz operators</strong> (U-Net convolutional kernels) and \(\mathcal{A}^{\mathrm{attn}}\) the class of <strong>self-attention kernels with relative positional structure</strong> (DiTs). Write \(\sqsubseteq^{\mathrm{bias}}\) as “is a constrained instance of (via inductive-bias constraints)”<d-cite key="hornTranslationalEquivarianceKernelizable2021"></d-cite><d-cite key="taiMathematicalExplanationUNet2024"></d-cite>.</p> \[\boxed{ \mathcal{C}^{\mathrm{conv}}_{G}\ \sqsubseteq^{\mathrm{bias}}\ \mathcal{A}^{\mathrm{attn}} }\] <p>In plain terms, <strong>convolution is a simplified, efficient expression of attention</strong> obtained by enforcing fixed translation symmetry, parameter tying, and locality<d-cite key="ramachandranStandAloneSelfAttentionVision2019"></d-cite><d-cite key="cordonnierRelationshipSelfAttentionConvolutional2020"></d-cite><d-cite key="changConvolutionsSelfAttentionReinterpreting2021"></d-cite><d-cite key="choiGraphConvolutionsEnrich2024"></d-cite><d-cite key="joshiTransformersAreGraph2025"></d-cite>; removing these constraints yields attention <strong>without a hard-coded translation prior</strong>, allowing DiTs to <em>learn</em> which symmetries and long-range relations matter at scale. This inclusion explains the empirical shift under modern hardware and datasets: attention strictly generalizes convolution while retaining it as an efficient special case, delivering smoother scaling laws and higher semantic “bandwidth” per denoising step. In practice, this is also a story of hardware path dependence: attention’s dense-matrix primitives align with contemporary accelerators and compiler stacks, effectively “winning” the hardware lottery <d-cite key="hookerHardwareLottery2021"></d-cite>. And, echoing the Bitter Lesson<d-cite key="richsuttonBitterLesson2019"></d-cite>, as data and compute grow, general methods with fewer hand-engineered priors dominate—making attention’s strict generalization of convolution the natural backbone at scale.</p> <h2 id="further-discussion">Further Discussion</h2> <h3 id="from-text-to-image-generation-to-real-world-applications">From Text-to-Image Generation to Real-World Applications</h3> <p>Text-to-image is now genuinely strong; the next wave is about <strong>conditioning existing pixels</strong> rather than generating from scratch—turning models into reliable editors that honor what must stay and change only what’s asked. This means prioritizing downstream tasks like image editing, inpainting/outpainting, image-to-image restyling, and structure- or reference-guided synthesis (edges, depth, layout, style, identity). The practical focus shifts from unconstrained novelty to controllable, faithful rewrites with tight mask adherence, robust subject/style preservation, and interactive latencies, so these systems plug cleanly into real creative, design, and industrial workflows.</p> <h3 id="diffusion-models-vs-auto-regressive-models">Diffusion Models vs. Auto-regressive Models</h3> <p>Diffusion models and autoregressive (AR) models represent two fundamentally different approaches to image generation, with the key distinction being that <strong>autoregressive models operate on discrete image tokens</strong> while <strong>diffusion models work with continuous representations</strong>. Autoregressive models like DALL-E <d-cite key="rameshZeroShotTexttoImageGeneration2021"></d-cite>, CogView <d-cite key="dingCogViewMasteringTexttoImage2021"></d-cite>, and CogView2 <d-cite key="dingCogView2FasterBetter2022"></d-cite> treat image generation as a sequence modeling problem, encoding images into discrete tokens using VQ-VAE <d-cite key="esserTamingTransformersHighResolution2021"></d-cite> or similar vector quantization methods, then autoregressively predicting the next token given previous tokens. This approach offers sequential generation with precise control and natural language integration, but suffers from slow generation, error accumulation, and discrete representation loss. In contrast, diffusion models operate directly on continuous pixel or latent representations, learning to reverse a gradual noise corruption process, which enables parallel generation, high-quality outputs, and flexible conditioning, though at the cost of computational overhead and less direct control. Recent advances have significantly improved autoregressive approaches: VAR <d-cite key="tianVisualAutoregressiveModeling2024"></d-cite> redefines autoregressive learning as coarse-to-fine “next-scale prediction” and achieves superior performance compared to diffusion transformers, while Infinity <d-cite key="hanInfinityScalingBitwise2025"></d-cite> demonstrates effective scaling of bitwise autoregressive modeling for high-resolution synthesis. Additionally, MAR <d-cite key="liAutoregressiveImageGeneration2024a"></d-cite> bridges the gap between paradigms by adopting diffusion loss for autoregressive models, enabling continuous-valued autoregressive generation without vector quantization. Recent work has also explored hybrid approaches that combine both paradigms: HunyuanImage 3.0 <d-cite key="caoHunyuanImage30Technical2025"></d-cite> and BLIP3-o <d-cite key="chenBLIP3oFamilyFully2025"></d-cite> demonstrate unified multimodal models within autoregressive frameworks while incorporating diffusion-inspired techniques, while OmniGen <d-cite key="xiaoOmniGenUnifiedImage2024"></d-cite> and OmniGen2 <d-cite key="wuOmniGen2ExplorationAdvanced2025"></d-cite> use diffusion models as backbones for unified generation capabilities.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[A comprehensive analysis of how diffusion model architectures evolved from U-Net backbones to Diffusion Transformers, transforming text-to-image generation capabilities.]]></summary></entry><entry><title type="html">Sample Blog Post</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/distill-example/" rel="alternate" type="text/html" title="Sample Blog Post"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/distill-example</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/distill-example/"><![CDATA[<p>Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling.</p> <h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\] <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. To include images in your submission in this way, you must do something like the following:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include figure.liquid path="assets/img/2026-04-27-distill-example/iclr.png" class="img-fluid" %}
</code></pre></div></div> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/iclr-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/iclr-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/iclr-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/iclr.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To ensure that there are no namespace conflicts, you must save your asset to your unique directory <code class="language-plaintext highlighter-rouge">/assets/img/2025-04-27-[SUBMISSION NAME]</code> within your submission.</p> <p>Please avoid using the direct markdown method of embedding images; they may not be properly resized. Some more complex ways to load images (note the different styles of the shapes/shadows):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/9-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/9-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/9-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/7-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/7-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/8-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/8-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/8.jpg" class="img-fluid z-depth-2" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/10-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/10-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/10-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/10.jpg" class="img-fluid z-depth-2" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/11-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/11-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/11-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/11.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/12-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/12-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/12-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/12.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/7-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/7-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/7.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="interactive-figures">Interactive Figures</h3> <p>Here’s how you could embed interactive figures that have been exported as HTML files. Note that we will be using plotly for this demo, but anything built off of HTML should work (<strong>no extra javascript is allowed!</strong>). All that’s required is for you to export your figure into HTML format, and make sure that the file exists in the <code class="language-plaintext highlighter-rouge">assets/html/[SUBMISSION NAME]/</code> directory in this repository’s root directory. To embed it into any page, simply insert the following code anywhere into your page.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include [FIGURE_NAME].html %}
</code></pre></div></div> <p>For example, the following code can be used to generate the figure underneath it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="n">lat</span><span class="o">=</span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="sh">'</span><span class="s">Magnitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mapbox_style</span><span class="o">=</span><span class="sh">"</span><span class="s">stamen-terrain</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="sh">'</span><span class="s">./assets/html/2026-04-27-distill-example/plotly_demo_1.html</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>And then include it with the following:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"l-page"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;iframe</span>
    <span class="na">src=</span><span class="s">"{{ 'assets/html/2026-04-27-distill-example/plotly_demo_1.html' | relative_url }}"</span>
    <span class="na">frameborder=</span><span class="s">"0"</span>
    <span class="na">scrolling=</span><span class="s">"no"</span>
    <span class="na">height=</span><span class="s">"600px"</span>
    <span class="na">width=</span><span class="s">"100%"</span>
  <span class="nt">&gt;&lt;/iframe&gt;</span>
<span class="nt">&lt;/div&gt;</span>

</code></pre></div></div> <p>Voila!</p> <div class="l-page"> <iframe src="/2026/assets/html/2026-04-27-distill-example/plotly_demo_1.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <h2 id="citations">Citations</h2> <p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas.</p> <p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it.</p> <p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p> <hr/> <h2 id="footnotes">Footnotes</h2> <p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p> <hr/> <h2 id="code-blocks">Code Blocks</h2> <p>This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag:</p> <p>{% highlight c++ linenos %} <br/> code code code <br/> {% endhighlight %}</p> <p>The keyword <code class="language-plaintext highlighter-rouge">linenos</code> triggers display of line numbers. You can try toggling it on or off yourself below:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>

<span class="p">}</span></code></pre></figure> <hr/> <h2 id="diagrams">Diagrams</h2> <p>This theme supports generating various diagrams from a text description using <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid.js</a> directly. Below, we generate examples of such diagrams using <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid</a> syntax.</p> <p><strong>Note:</strong> To enable mermaid diagrams, you need to add the following to your post’s front matter:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">mermaid</span><span class="pi">:</span>
  <span class="na">enabled</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">zoomable</span><span class="pi">:</span> <span class="kc">true</span> <span class="c1"># optional, for zoomable diagrams</span>
</code></pre></div></div> <p>The diagram below was generated by the following code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>```mermaid
sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
```
</code></pre></div></div> <pre><code class="language-mermaid">sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
</code></pre> <hr/> <h2 id="tweets">Tweets</h2> <p>An example of displaying a tweet:</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>An example of pulling from a timeline:</p> <div class="jekyll-twitter-plugin"><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a></p> <hr/> <h2 id="blockquotes">Blockquotes</h2> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <hr/> <h2 id="layouts">Layouts</h2> <p>The main text column is referred to as the body. It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p> <div class="fake-img l-body"> <p>.l-body</p> </div> <p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p> <div class="fake-img l-page"> <p>.l-page</p> </div> <p>All of these have an outset variant if you want to poke out from the body text a little bit. For instance:</p> <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> <p>Occasionally you’ll want to use the full browser width. For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>. You can also inset the element a little from the edge of the browser by using the inset variant.</p> <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> <p>The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code>-sized text except on mobile screen sizes.</p> <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <hr/> <h2 id="other-typography">Other Typography?</h2> <p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p> <p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p> <p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p> <p>Strikethrough uses two tildes. <del>Scratch this.</del></p> <ol> <li>First ordered list item</li> <li>Another item</li> </ol> <ul> <li>Unordered sub-list.</li> </ul> <ol> <li>Actual numbers don’t matter, just that it’s a number <ol> <li>Ordered sub-list</li> </ol> </li> <li> <p>And another item.</p> <p>You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p> <p>To have a line break without a paragraph, you will need to use two trailing spaces. Note that this line is separate, but within the same paragraph. (This is contrary to the typical GFM line break behavior, where trailing spaces are not required.)</p> </li> </ol> <ul> <li> <p>Unordered lists can use asterisks</p> </li> <li> <p>Or minuses</p> </li> <li> <p>Or pluses</p> </li> </ul> <p><a href="https://www.google.com">I’m an inline-style link</a></p> <p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p> <p><a href="https://www.mozilla.org">I’m a reference-style link</a></p> <p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p> <p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p> <p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p> <p>URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes example.com (but not on Github, for example).</p> <p>Some text to show that the reference links can follow later.</p> <p>Here’s our logo (hover to see the title text):</p> <p>Inline-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1"/></p> <p>Reference-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2"/></p> <p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting</span><span class="sh">"</span>
<span class="nf">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting.
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div> <p>Colons can be used to align columns.</p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p>There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don’t need to make the raw Markdown line up prettily. You can also use inline Markdown.</p> <table> <thead> <tr> <th>Markdown</th> <th>Less</th> <th>Pretty</th> </tr> </thead> <tbody> <tr> <td><em>Still</em></td> <td><code class="language-plaintext highlighter-rouge">renders</code></td> <td><strong>nicely</strong></td> </tr> <tr> <td>1</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <blockquote> <p>Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.</p> </blockquote> <p>Quote break.</p> <blockquote> <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p> </blockquote> <p>Here’s a line for us to start with.</p> <p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p> <p>This line is also a separate paragraph, but… This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.]]></summary></entry><entry><title type="html">Research Directions in Multimodal Chain-of-Thought (MCoT) with Sketching</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/mcot-sketching/" rel="alternate" type="text/html" title="Research Directions in Multimodal Chain-of-Thought (MCoT) with Sketching"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/mcot-sketching</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/mcot-sketching/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Drawing and sketching are cognitive tools that humans use not only to express and communicate thoughts, but also to generate new ones <d-cite key="Fan"></d-cite>. For this matter, we would like to equip any intelligent system with the same ability to improve and help it communicate its reasoning. First steps in this direction have been proposed within the field of Multimodal Chain-of-Thought (MCoT) where reasoning steps are enriched with data from different modalities, such as visuals. Therefore, future research on sketching should advance the design of MCoT reasoning strategies. Improving Multimodal Large Language Models (MLLMs) that perform such cross-modal reasoning is also relevant.</p> <h2 id="motivation-to-incorporate-drawing-capabilities-into-ai">Motivation to incorporate drawing capabilities into AI</h2> <p>Humans express and communicate ideas visually through drawing and sketching, which is a quick and loose form of drawing. Drawing is a representation of thought, but also an activity that can support ongoing cognition <d-cite key="Fan"></d-cite>. Drawing and sketching precede writing: The first documented drawings date back as far as 64,000 years <d-cite key="Hoffmann"></d-cite>. For that reason, Fan et al. <d-cite key="Fan"></d-cite> argue that drawing is one of the most enduring and versatile cognitive tools from which humans have benefited.</p> <p>One explanation for the power of drawing and sketching can be derived from cognitive enhancement and offloading strategies. According to Morrison and Richmond <d-cite key="Morrison2020"></d-cite>, technologies are used as external memories, facilitating other tasks by freeing up memory. Similarly, Osiurak et al. <d-cite key="Osiurak2018"></d-cite> show that tools such as maps can extend human’s cognitive abilities.</p> <p>Given the relevance of drawing and sketching for human thought, expression, and communication, we would want to equip any AI with the capability to also use this tool to advance and share its own ideas. Sketching can not only be a window into how AI models process information, but it is fair to assume that it can also support their reasoning.</p> <p>Reasoning in large language models (LLMs) has been greatly improved with in-context learning (ICL) <d-cite key="Min2022RethinkingTR"></d-cite> and Chain-of-Thought (CoT) techniques <d-cite key="Nye, Wei"></d-cite>. ICL helps models with additional information added to the input to find appropriate responses for a given task. With CoT, the contextual information is specifically extended by a simulation of human reasoning steps, where a task is divided into subtasks for which intermediate solutions are given so that the model can derive its final answer from them. This can be achieved by eliciting reasoning through prompting, as with ’think step-by-step’ prompts (Zero-Shot-CoT <d-cite key="Kojima2022LargeLM"></d-cite>), or by providing the model with an explicit reasoning demonstration (also called a rationale) for a given problem (Few-Shot-CoT <d-cite key="Wei"></d-cite>).</p> <p>CoT has been extended with multimodal information <d-cite key="Wang2024ExploringTR, Wang2025MultimodalCR"></d-cite> where models receive more than text to guide them toward a correct answer. This information can consist of visual, auditory, or spatio-temporal data. Sketches would be additional visual information. They could also help models to offload complex tasks and retain intermediate memories, for example, of subtasks. Therefore, an implementation of the capability to sketch in order to enhance models’ reasoning abilities should expand existing research in MCoT. A detailed account of MCoT is given in <strong>Appendix A</strong>.</p> <h2 id="related-work">Related work</h2> <p>Several recent approaches explore MCoT reasoning, though most do not fully integrate sketch generation into the reasoning process.</p> <p>Zhang et al. <d-cite key="Zhang2023MultimodalCR"></d-cite> propose a two-stage framework for multiple-choice reasoning for text and image inputs where a FLAN-AlpacaBase model <d-cite key="taori_alpaca_2023, Zheng2023JudgingLW"></d-cite> first produces a rationale, then derives the answer. Fusing text and image features from the input improves performance, but the system cannot generate new visual content. This limits applicability to reasoning scenarios that benefit from active visual exploration, such as diagram construction in geometry or mechanical design tasks.</p> <p>Meng et al. <d-cite key="Meng2023ChainOI"></d-cite> extend CoT by having an LLM produce symbolic sketch-like diagrams (e.g., with SVG), rendered into images and re-encoded for reasoning. Their ’think image by image’ approach helps, for example, with geometric tasks. However, this gain comes at the cost of operational complexity: the pipeline depends on separate LLMs, rendering engines, and encoders, creating latency and integration challenges. Unified MLLMs avoid such fragmentation and may better support generalization by learning a shared latent space for both text and sketches.</p> <p>In contrast to the previous two approaches, Liao et al. <d-cite key="Liao2025ImageGenCoTET"></d-cite> fine-tune unified MLLMs (SEED-LLaMA <d-cite key="Ge2023MakingLS"></d-cite> and SEED-X <d-cite key="Ge2024SEEDXMM"></d-cite>) on their ImageGen-CoT dataset. Reasoning steps of their models precede image generation. Test-time scaling is applied to select better outputs. While they demonstrate high-quality image generation, their evaluation focuses on aesthetics and relevance rather than measurable reasoning improvement. For reasoning-centric applications, visual fidelity without explicit reasoning gains may be insufficient.</p> <p>Hu et al. <d-cite key="Hu2024VisualSS"></d-cite> and Vinker et al. <d-cite key="Vinker2024SketchAgentLS"></d-cite> develop agentic strategies (Sketchpad, Sketchagent) where models like GPT-4o <d-cite key="Hurst2024GPT4oSC"></d-cite> or Claude3.5-Sonnet <d-cite key="TheC3"></d-cite> can decide to produce or modify sketches during problem-solving by leveraging external vision models, Python or a domain-specific language (DSL) for sketches. Models with Sketchpad iterate over a ’thought’, ’action’ (to inject sketches), and ’observation’ pattern. With this approach, Hu et al. <d-cite key="Hu2024VisualSS"></d-cite> show that allowing models to decide to insert sketches during reasoning leads to notable performance gains. However, the framework relies on external vision models to rather enhance or dissect images and a Python sketch representation, which may not capture the nuances of freehand or abstract sketches common in human reasoning.</p> <p>A truly multimodal approach for sketches would not use Python or DSLs to ’implicitly’ generate figures that the model ingests as textual input. However, few multimodal datasets that combine visuals with rationales exist. While QuickDraw <d-cite key="Jongejan_quick_draw"></d-cite> provides scale and diversity in sketch data, its lack of accompanying rationales prevents multimodal alignment learning. ScienceQA <d-cite key="lu2022learn"></d-cite> and ImageGen-CoT <d-cite key="Liao2025ImageGenCoTET"></d-cite> offer strong rationale-image pairs, but the absence of sketches means they primarily serve full-image reasoning rather than schematic reasoning. This gap suggests that the field currently lacks a dataset that balances sketch simplicity with reasoning, a pairing that could uniquely advance MCoT.</p> <p>Overall, existing MCoT work shows that visual information, including sketches, can aid reasoning. However, limitations remain: most systems either consume but do not create sketches, focus on image quality rather than reasoning improvement, or require orchestration of multiple models instead of unified generation. Furthermore, appropriate datasets with sketches in combination with rationales are lacking.</p> <h2 id="future-research-for-mcot-with-sketching">Future research for MCoT with sketching</h2> <p>Given the power of visual information for reasoning tasks, as shown by <d-cite key="Zhang2023MultimodalCR, Meng2023ChainOI, Liao2025ImageGenCoTET, Hu2024VisualSS"></d-cite>, some of the shortcomings of existing MCoT approaches can be addressed to better incorporate sketching in future research.</p> <h3 id="creating-a-new-mcot-sketch-dataset">Creating a new MCoT sketch dataset</h3> <p>To facilitate the training of MLLMs, the lack of an appropriate dataset with sketching and rationales is a limitation.</p> <p>Sketch data should be gathered and grouped within different categories, depending on the downstream task (consider Figure 1). In experimental studies with humans, Huey et al. <d-cite key="Huey2023VisualEP"></d-cite> point out that drawings differ according to their intended goal: visual explanations by the participants emphasized moving and interactive parts, while their visual depictions focused on salient features. Hu et al. <d-cite key="Hu2024VisualSS"></d-cite> show that adding auxiliary lines to geometric figures helps multimodal models such as GPT-4o to infer correct answers about these figures. Fan et al. <d-cite key="Fan"></d-cite> highlight that not all drawings are faithful depictions, but can also be abstractions whose meanings are conveyed by cultural conventions.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-mcot-sketching/sketches-480.webp 480w,/2026/assets/img/2026-04-27-mcot-sketching/sketches-800.webp 800w,/2026/assets/img/2026-04-27-mcot-sketching/sketches-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-mcot-sketching/sketches.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Figure 1:</strong> Different types of sketches and drawings: (a) depicts a geometric form that has an auxiliary line, (b) emphasizes moving parts of a machine, (c) depicts the same machine in more detail, (d) represents figures from tetris whose next moves are indicated with arrows, (e) is a conventional sketch of a heart that does not resemble actual human hearts.</p> <p>To integrate sketches into a CoT, training data should not only consist of images of drawings and sketches, but combine these with textual rationales. This would enable multimodal alignment between visual and linguistic reasoning steps. A typical template for this data could consist of instruction <em>I</em>, query <em>Q</em>, rationale <em>R</em>, and answer <em>A</em> where we could further divide <em>R</em> into ’thought’, ’sketch’, and ’observation’ with respective special tokens to guide the model, loosely following Hu et al. <d-cite key="Hu2024VisualSS"></d-cite>. An example template is given in <strong>Appendix B</strong>. Since ScienceQA and ImageGen-CoT already pair images with rationales, they could be extended with sketches to strengthen visual-textual alignment for their tasks.</p> <h3 id="advancing-mcot-with-unified-mllms">Advancing MCoT with unified MLLMs</h3> <p>To avoid multi-model orchestration and to leverage potential transfer-learning effects, further advancing reasoning of MLLMs with sketches is a promising direction. However, there exist only a few MLLMs <d-cite key="Yu2023ScalingAM, Zhao2025R1OmniEO, Zhang2023MultimodalCR, swerdlow2025unidisc"></d-cite> that can potentially handle sketch-to-text as well as text-to-sketch tasks within a unified architecture (consider Figure 2). The majority of current approaches such as Sketchpad pair VLMs such as Flamingo <d-cite key="Alayrac2022FlamingoAV"></d-cite>, PaLM-E <d-cite key="Driess2023PaLMEAE"></d-cite>, LLAVA <d-cite key="Liu2023VisualIT"></d-cite>, GPT-4o <d-cite key="Hurst2024GPT4oSC"></d-cite>, or Claude3-Opus and Claude3.5-Sonnet <d-cite key="TheC3"></d-cite> with text-to-image models.</p> <p>Unified MLLMs can be divided into autoregressive (AR) and diffusion-based MLLMs. For example, CM3Leon <d-cite key="Yu2023ScalingAM"></d-cite> from Meta is a Transfomer-based AR decoder that can generate both text and images. It is built on the CM3 model <d-cite key="Aghajanyan2022CM3AC"></d-cite>. CM3Leon has been trained on text-guided image editing, image-to-image grounding tasks where visual features can be derived from images, and text-to-image generations.</p> <p>Swerdlow et al. <d-cite key="swerdlow2025unidisc"></d-cite> introduce a unified multimodal discrete diffusion model (UniDisc). While the model’s architecture consists of a Transformer (bidirectional) decoder, its training goal is not to auto-regressively predict the next tokens in a sequential manner (e.g., left to right for text or top to bottom for image patch rasters), but to predict the distribution of tokens via a denoising process that allows parallel predictions as well as later refinements. The training of UniDisc is realized with a denoising process of corrupted inputs (masking). In contrast to continuous diffusion models, Swerdlow et al. <d-cite key="swerdlow2025unidisc"></d-cite> use discrete noising and denoising for both images and texts. Swerdlow et al. <d-cite key="swerdlow2025unidisc"></d-cite> show that UniDisc outperforms the same architecture without a diffusion objective with respect to image and text classification tasks. The model is also capable of inpainting and infilling missing parts of an input, which no AR model can do. However, these performance gains come at a cost: UniDisc requires 13.2 times longer than its AR counterpart to reach equivalent loss levels <d-cite key="swerdlow2025unidisc"></d-cite>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-mcot-sketching/mcot-480.webp 480w,/2026/assets/img/2026-04-27-mcot-sketching/mcot-800.webp 800w,/2026/assets/img/2026-04-27-mcot-sketching/mcot-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-mcot-sketching/mcot.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Figure 2:</strong> MCoT involving sketches with a Multimodal Large Language Model (MLLM). Black arrows represent sequential auto-regressive processing, while blue arrows illustrate the bidirectionality of diffusion models. The model’s reasoning is guided by special tokens, such as &lt;think&gt;.</p> <p>Models like UniDisc provide an interesting model class for MCoT. While current diffusion language models (DLMs) might not rival AR LLMs due to training inefficiencies <d-cite key="swerdlow2025unidisc"></d-cite> or speed <d-cite key="dream2025"></d-cite>, the strength of multimodal DLMs in handling and generating multimodal data – as shown by Swerdlow et al. <d-cite key="swerdlow2025unidisc"></d-cite> – warrants further research. Their ability to inpaint and infill would be particularly helpful for amending visualizations, which is a core aspect of explanatory sketching. Research in this direction could be informed by Diffusion-of-Thought (DoT) proposed by Ye et al. <d-cite key="Ye2024DiffusionOT"></d-cite>, who fine-tune a DLM for CoT. However, diffusion models require a fixed output size. This is a challenge that needs to be addressed to allow versatile reasoning over different tasks.</p> <h3 id="improving-mcot-with-reinforcement-learning-rl-and-test-time-scaling">Improving MCoT with reinforcement learning (RL) and test-time scaling</h3> <p>Existing work on MCoT <d-cite key="Zhang2023MultimodalCR, Meng2023ChainOI, Liao2025ImageGenCoTET"></d-cite> has mainly relied on supervised fine-tuning (SFT). However, other work in <em>reasoning</em> has shown that RL leads to improvements <d-cite key="DeepSeekAI2025DeepSeekR1IR, Ranaldi2025MultilingualRV, Zhao2025R1OmniEO"></d-cite>. Therefore, MCoT could be advanced with Direct Preference Optimization (DPO) <d-cite key="Rafailov2023DirectPO"></d-cite>, Reinforcement Learning with Verifiable Rewards (RLVR) <d-cite key="DeepSeekAI2025DeepSeekR1IR"></d-cite> and Group Relative Policy Optimization (GRPO) <d-cite key="Shao2024DeepSeekMathPT"></d-cite> strategies. One straight-forward application would be to use RLVR with GRPO, following Deepseek’s R1 <d-cite key="DeepSeekAI2025DeepSeekR1IR"></d-cite>, to reward accuracy (\(R_{acc}\)) and format (\(R_{format}\)) for rationales and answers based on generated sketches.</p> <p>An appropriate reward for the generation of sketches could leverage AR-GRPO for autoregressive MLLMs <d-cite key="Yuan2025ARGRPOTA"></d-cite>. AR-GRPO realizes rewards for the generation of images with a multi-faceted reward function that ensures (a) consistency with the textual input condition through CLIP <d-cite key="Radford2021LearningTV"></d-cite> and Human Preference Score v2 <d-cite key="Wu2023HumanPS"></d-cite>, (b) image quality with MANIQA <d-cite key="Yang2022MANIQAMA}"></d-cite>, and (c) a further realism reward through a VLM, such as Qwen2.5-VL-3B-Instruct <d-cite key="Bai2025Qwen25VLTR"></d-cite>. This function is used with GRPO to improve the quality of generated images. Since the proposed rewards by Yuan et al. <d-cite key="Yuan2025ARGRPOTA"></d-cite> focus on overall quality, a specific reward should be conceived for sketches. For example, a sketch can consist of a hierarchy of strokes whose meaning can be of different importance. It would be interesting to incorporate this somehow into the reward: Should sketches with a limited amount of strokes be prioritized?</p> <p>In the wake of Liao et al. <d-cite key="Liao2025ImageGenCoTET"></d-cite>, existing MCoT could be further improved with Test-time scaling methods, sampling more CoTs and sketches to select the best candidates with an appropriate scoring method. This approach could also be used with agentic frameworks that pair VLMs with image generators and would not require any additional training of the models.</p> <p>Beyond standard accuracy on downstream tasks, evaluation should measure how sketches contribute to the reasoning process. This includes interpretability (e.g., can a human follow the model’s reasoning with a sketch?), task completion time (one of the biggest bottlenecks because image generation requires many tokens), error localization, and robustness under noisy or incomplete inputs. Additionally, user studies could assess subjective clarity and helpfulness of generated sketches.</p> <h2 id="impact">Impact</h2> <p>MLLMs with sketching would have an impact on AI in different domains. For example, agentic systems such as Auto-GUI <d-cite key="Zhang2023YouOL"></d-cite> that interact with graphical user interfaces or websites could be enhanced by providing them with additional visual information with sketches. Similarly, embodied AI systems, such as EmbodiedGPT <d-cite key="Mu2023EmbodiedGPTVP"></d-cite> whose backbone uses a combination of vision and language models that help navigate the real world, could reason about their surroundings using sketches. MLLMs for STEM education could also benefit from the ability to make their reasoning more transparent with additional drawings as proposed in Meng et al. <d-cite key="Meng2023ChainOI"></d-cite>. In sum, sketching would help all reasoning models not only to enhance their thoughts, but also communicate them with more than one modality.</p> <p>As with language, sketches are not neutral representations. The ability of AI systems to generate and reason with sketches introduces risks of cultural bias, visual misrepresentation, and domain-specific inaccuracies. For example, the “heart” symbol in Figure 1(e) is globally recognized in popular culture but anatomically incorrect; in medical education, reasoning over such a schematic could reinforce misconceptions. Similar issues may arise if models default to culturally specific diagrammatic conventions, omit critical features due to dataset biases, or overgeneralize from training examples.</p> <p>Ethical safeguards should address the entire MCoT-with-sketching workflow. Dataset curation must ensure diversity of styles, cultural perspectives, and schematic conventions. Annotation guidelines should clarify the intended use and accuracy requirements of sketches. Model evaluation should include bias detection for visual outputs, alongside interpretability checks so users can trace how a sketch influenced reasoning.</p> <h2 id="appendix-a-mcot-foundations">Appendix A MCoT foundations</h2> <p>Following Wang et al. <d-cite key="Wang2025MultimodalCR"></d-cite>, we can define prompt, instruction, query, answer, and rationale with \(P\) , \(I\), \(Q\), \(A\), and \(R\), which are all token sequences. A Chain-of-Thought (CoT) would be:</p> <p>\begin{equation} P_{CoT} = {I, (x_1, e_1, y_1), …, (x_n, e_n, y_n)} \end{equation}</p> <p>where \(x_i \in Q\) and \(y_i \in A\) are questions with corresponding answers and \(e_i \in R\) is an example rationale. The joint probability of generating an answer A and a rationale R given the prompt \(P_{CoT}\) and a query \(Q\) would be <d-cite key="Wang2025MultimodalCR"></d-cite>:</p> <p>\begin{equation} p(A, R |P_{CoT}, Q) = p(R |P_{CoT}, Q) \cdot p(A |P_{CoT}, Q, R) \end{equation}</p> <p>where the model should output rationale \(R\) with the tokens \(r_1, ..., r_i\) before arriving at the answer \(A\) consisting of the tokens \(a_1, ..., a_i\). The goal in training a reasoning model \(F\) is to jointly maximize the likelihood of equation (2).</p> <p>Finally, all components \(P\), \(Q\), \(A\), and \(R\) can be enriched with multimodal information \(\mathcal{M}\). For example with MCoT, a rationale \(R\) should handle \(\mathcal{M}\) input and generate multimodal information (e.g., a sketch) as well as text \(T\), that is, \(R\in\{M, M\oplus T\}\) <d-cite key="Wang2025MultimodalCR"></d-cite>.</p> <h2 id="appendix-b-mcot-template">Appendix B MCoT template</h2> <p><code class="language-plaintext highlighter-rouge"> { "instruction": "Find proofs for geometry problems.", "query": "Prove the angles of ABC provided in the attached image sum to 180. &lt;image&gt; VT_011 VT_115 VT_563 VT_101 ... VT_909 &lt;/image&gt;", "rationale": "&lt;think&gt; I need to figure out how ABC are related in the image. The image shows a triangle. I need to prove that the angles of the triangle sum to 180. To find an answer, I draw a triangle: Let's call it ABC. &lt;sketch&gt; VT_421 VT_105 VT_983 VT_002 ... VT_778 &lt;/sketch&gt; I extend the sides from A to B, from A to C, and from B to C. &lt;sketch&gt; VT_421 VT_105 VT_983 VT_001 ... VT_708 &lt;/sketch&gt; I draw a line parallel to AB through point C. &lt;sketch&gt; VT_420 VT_105 VT_983 VT_001 ... VT_718 &lt;/sketch&gt; &lt;observe&gt; The angles at point C created by the parallel line correspond to the interior angles at points A and B. When I add those angles up, they form a straight line at point C, which measures 180. Since those angles correspond exactly to the three interior angles of the triangle, the sum of the interior angles is 180. &lt;/observe&gt; This proof follows from the alternate interior angles theorem. &lt;/think&gt;", "answer": "The alternate interior angles theorem shows that all angles at point C created by the parallel line sum to 180. They further correspond to the interior angles at points A and B. Therefore, the angles of ABC provided in the attached image sum to 180." } </code></p> <p>MCoT template with instruction \(I\), query \(Q\), rationale \(R\), and answer \(A\) where \(R\) is further divided into “thought”, “sketch”, and “observation” with respective special tokens to guide the model. VT_n tokens correspond to image tokens.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[This article explores adding sketching to Multimodal Chain-of-Thought (MCoT)reasoning to enhance AI capabilities. It reviews past methods, identifies key gaps such as the lack of sketch-rationale datasets, and proposes advancing the field through targeted data collection, unified multimodal models, and reinforcement learning. Ethical considerations include mitigating cultural bias and visual misrepresentation in generated sketches.]]></summary></entry><entry><title type="html">Getting SAC to Work on a Massive Parallel Simulator: An RL Journey With Off-Policy Algorithms</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/sac-massive-sim/" rel="alternate" type="text/html" title="Getting SAC to Work on a Massive Parallel Simulator: An RL Journey With Off-Policy Algorithms"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/sac-massive-sim</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/sac-massive-sim/"><![CDATA[ <p>Spoiler alert: <a href="#appendix-affected-paperscode">quite a few papers/code</a> are affected by the problem described below.</p> <p>This post is divided into two main parts. The first part analyzes why SAC does not work out of the box in Isaac Sim environments (until the <a href="#quick-fix">quick fix section</a>). The <a href="#tuning-for-speed-part-ii">second part</a> discusses how to tune SAC for speed and make it perform as good as PPO.</p> <h2 id="a-suspicious-trend-ppo-ppo-ppo-">A Suspicious Trend: PPO, PPO, PPO, …</h2> <p>The story begins a few months ago when I saw another paper using the same recipe for learning locomotion: train a PPO<d-cite key="schulman2017proximal"></d-cite> agent in simulation using thousands of environments in parallel and domain randomization, then deploy it on the real robot. This recipe has become the standard since 2021, when ETH Zurich and NVIDIA showed that it was possible to learn locomotion in minutes on a single workstation<d-cite key="rudin2022learning"></d-cite>. The codebase and the simulator (called Isaac Gym<d-cite key="makoviychuk2021isaac"></d-cite> at that time) that were published became the basis for much follow-up work<d-footnote>Like the <a href="https://www.youtube.com/watch?v=7_LW7u-nk6Q">BD-1 Disney robot</a></d-footnote>.</p> <p>As an RL researcher interested in learning directly on real robots, I was curious and suspicious about one aspect of this trend: why is no one trying an algorithm other than PPO<d-footnote>I was not the only one asking why SAC doesn't work: <a href="https://forums.developer.nvidia.com/t/poor-performance-of-soft-actor-critic-sac-in-omniverseisaacgym/266970">nvidia forum</a> <a href="https://www.reddit.com/r/reinforcementlearning/comments/lcx0cm/scaling_up_sac_with_parallel_environments/">reddit1</a> <a href="https://www.reddit.com/r/reinforcementlearning/comments/12h1faq/isaac_gym_with_offpolicy_algorithms">reddit2</a></d-footnote>? PPO benefits from fast and parallel environments<d-cite key="berner2019dota"></d-cite>, but PPO is not the only deep reinforcement learning (DRL) algorithm for continuous control tasks, and there are alternatives like SAC<d-cite key="haarnoja2018soft"></d-cite> or TQC<d-cite key="kuznetsov2020tqc"></d-cite> that can lead to better performance<d-cite key="huang2023openrlbenchmark"></d-cite>.</p> <p>So I decided to investigate why practitioners do not use these off-policy algorithms, and maybe why they don’t work with massively parallel simulators.</p> <h2 id="why-it-matters---fine-tuning-on-real-robots">Why It Matters? - Fine Tuning on Real Robots</h2> <p>If we could make SAC work with these simulators, then it would be possible to train in simulation and fine-tune on the real robot using the same algorithm (PPO is too sample-inefficient to train on a single robot).</p> <p>By using other algorithms, it might also be possible to get better performance. Finally, it is always good to better understand what works or not and why. As researchers, we tend to publish only positive results, but a lot of valuable insights are lost in our unpublished failures.</p> <h2 id="the-path-of-least-resistance-hypothesis">(The Path of Least Resistance) Hypothesis</h2> <p>Before digging any further, I had some hypotheses as to why PPO was the only algorithm used:</p> <ul> <li>PPO is fast to train (in terms of computation time) and was tuned for the massively parallel environment.</li> <li>As researchers, we tend to take the path of least resistance and build on proven solutions (the original training code is open source, and the simulator is freely available) to get new, interesting results<d-footnote>Yes, we tend to be lazy.</d-footnote>.</li> <li>Some peculiarities in the environment design may favor PPO over other algorithms. In other words, the massively parallel environments might be optimized for PPO.</li> <li>SAC/TQC and derivatives are tuned for sample efficiency, not fast wall clock time. In the case of massively parallel simulation, what matters is how long it takes to train, not how many samples are used. They probably need to be tuned/adjusted for this new setting.</li> </ul> <p>Note: During my journey, I will be using <a href="https://github.com/DLR-RM/stable-baselines3">Stable-Baselines3</a><d-cite key="raffin2021sb3"></d-cite> and its fast Jax version <a href="https://github.com/araffin/sbx">SBX</a>.</p> <h2 id="the-hunt-begins">The Hunt Begins</h2> <p>There are now many massively parallel simulators available (Isaac Sim, Brax, MJX, Genesis, …), here, I chose to focus on Isaac Sim because it was one of the first and is probably the most influential one.</p> <figure> <video src="https://b2drop.eudat.eu/public.php/dav/files/z5LFrzLNfrPMd9o/ppo_trained.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> <div class="caption"> A PPO agent trained on the <code>Isaac-Velocity-Flat-Unitree-A1-v0</code> locomotion task. <br/> Green arrow is the desired velocity, blue arrow represents the current velocity </div> <p>As with any RL problem, starting simple is the key to success <d-footnote>Also known as <a href="https://en.wikipedia.org/wiki/John_Gall_(author)#Gall's_law">Gall's law</a></d-footnote>. Therefore, I decided to focus on the <code class="language-plaintext highlighter-rouge">Isaac-Velocity-Flat-Unitree-A1-v0</code> locomotion task first, because it is simple but representative. The goal is to learn a policy that can move the Unitree A1 quadruped in any direction on flat ground, following a commanded velocity (the same way you would control a robot with a joystick). The agent receives information about its current task as input (joint positions, velocities, desired velocity, …) and outputs desired joint positions (12D vector, three joints per leg). The robot is rewarded for following the correct desired velocity (linear and angular) and for other secondary tasks (feet air time, smooth control, …). An episode ends when the robot falls over and is timed out after 1000 steps<d-footnote>The control loop runs at <a href="https://github.com/isaac-sim/IsaacLab/blob/f1a4975eb7bae8509082a8ff02fd775810a73531/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/velocity_env_cfg.py#L302">50 Hz</a>, so after 20 seconds</d-footnote>.</p> <p>To begin, I did some sanity checks. I ran PPO with the <a href="https://github.com/isaac-sim/IsaacLab/blob/f52aa9802780e897c184684d1cbc2025fafcef4a/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/a1/agents/sb3_ppo_cfg.yaml">tuned hyperparameters</a> found in the repository, and it was able to quickly solve the task. In 5 minutes, it gets an average episode return of ~30 (above an episode return of 15, the task is almost solved). Then I tried SAC and TQC, with default hyperparameters (and observation normalization), and, as expected, it didn’t work. No matter how long it was training, there was no sign of improvement.</p> <p>Looking at the simulation GUI, something struck me: the robots were making very large random movements. Something was wrong.</p> <figure> <video src="https://b2drop.eudat.eu/public.php/dav/files/z5LFrzLNfrPMd9o/limits_train.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> <div class="caption"> SAC out of the box on Isaac Sim during training. </div> <p>Because of the very large movements, my suspicion was towards what action the robot is allowed to take. Looking at the code, the RL agent commands a (scaled) <a href="https://github.com/isaac-sim/IsaacLab/blob/f1a4975eb7bae8509082a8ff02fd775810a73531/source/isaaclab/isaaclab/envs/mdp/actions/joint_actions.py#L134">delta</a> with respect to a default <a href="https://github.com/isaac-sim/IsaacLab/blob/f1a4975eb7bae8509082a8ff02fd775810a73531/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/velocity_env_cfg.py#L112">joint position</a>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Note desired_joint_pos is of dimension 12 (3 joints per leg)
</span><span class="n">desired_joint_pos</span> <span class="o">=</span> <span class="n">default_joint_pos</span> <span class="o">+</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">action</span>
</code></pre></div></div> <p>Then, let’s look at the action space itself (I’m using <code class="language-plaintext highlighter-rouge">ipdb</code> to have an interactive debugger):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">ipdb</span><span class="p">;</span> <span class="n">ipdb</span><span class="p">.</span><span class="nf">set_trace</span><span class="p">()</span>
<span class="o">&gt;&gt;</span> <span class="n">vec_env</span><span class="p">.</span><span class="n">action_space</span>
<span class="nc">Box</span><span class="p">(</span><span class="o">-</span><span class="mf">100.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">,</span> <span class="p">(</span><span class="mi">12</span><span class="p">,),</span> <span class="n">float32</span><span class="p">)</span>
</code></pre></div></div> <p>Ah ah! The action space defines continuous actions of dimension 12 (nothing wrong here), but the limits \([-100, 100]\) are surprisingly large, e.g., it allows a delta of +/- 1432 deg!! in joint angle when <a href="https://github.com/isaac-sim/IsaacLab/blob/f1a4975eb7bae8509082a8ff02fd775810a73531/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/a1/rough_env_cfg.py#L30">scale=0.25</a>, like for the Unitree A1 robot. To understand why normalizing the action space <a href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html">matters</a> (usually a bounded space in \([-1, 1]\)), we need to dig deeper into how PPO works.</p> <h2 id="ppo-gaussian-distribution">PPO Gaussian Distribution</h2> <p>Like many RL algorithms, PPO relies on a probability distribution to select actions<d-cite key="shengyi2022the37implementation"></d-cite>. During training, at each timestep, it samples an action \(a_t \sim N(\mu_\theta(s_t), \sigma^2)\) from a Gaussian distribution in the case of continuous actions<d-footnote>This is not true for the PPO implementation in Brax which uses a squashed Gaussian like SAC.</d-footnote>. The mean of the Gaussian \(\mu_\theta(s_t)\) is the output of the actor neural network (with parameters \(\theta\)) and the standard deviation is a <a href="https://github.com/DLR-RM/stable-baselines3/blob/55d6f18dbd880c62d40a276349b8bac7ebf453cd/stable_baselines3/common/distributions.py#L150">learnable parameter</a> \(\sigma\), usually <a href="https://github.com/leggedrobotics/rsl_rl/blob/f80d4750fbdfb62cfdb0c362b7063450f427cf35/rsl_rl/modules/actor_critic.py#L26">initialized</a> with \(\sigma_0 = 1.0\).</p> <p>This means that at the beginning of training, most of the sampled actions will be in \([-3, 3]\) (from the <a href="https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule">Three Sigma Rule</a>):</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-sac-massive-sim/gaussian.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-sac-massive-sim/gaussian.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> The initial Gaussian distribution used by PPO for sampling actions. </div> <p>Back to our original topic, because of the way \(\sigma\) is initialized, if the action space has large bounds (upper/lower bounds » 1), PPO will almost never sample actions near the limits. In practice, the actions taken by PPO will be far from them. Now, let’s compare the initial PPO action distribution with the Unitree A1 action space:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-sac-massive-sim/gaussian_large_bounds.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-sac-massive-sim/gaussian_large_bounds.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> The same initial Gaussian distribution, but with the perspective of the Unitree A1 action space $$[-100, 100]$$ </div> <p>For reference, we can plot the action distribution of PPO after training<d-footnote>The code to record and plot action distribution is in the <a href="#appendix-plot-action-distribution">Appendix</a></d-footnote>: </p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-sac-massive-sim/dist_actions_trained_ppo.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-sac-massive-sim/dist_actions_trained_ppo.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Distribution of actions for PPO after training (on 64 000 steps). </div> <p>The min/max values per dimension:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;</span> <span class="n">actions</span><span class="p">.</span><span class="nf">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nf">array</span><span class="p">([</span><span class="o">-</span><span class="mf">3.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.</span> <span class="p">,</span> <span class="o">-</span><span class="mf">3.9</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.8</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.8</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.7</span><span class="p">])</span>
<span class="o">&gt;&gt;</span> <span class="n">actions</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nf">array</span><span class="p">([</span> <span class="mf">3.2</span><span class="p">,</span>  <span class="mf">2.8</span><span class="p">,</span>  <span class="mf">2.7</span><span class="p">,</span>  <span class="mf">2.8</span><span class="p">,</span>  <span class="mf">2.9</span><span class="p">,</span>  <span class="mf">2.7</span><span class="p">,</span>  <span class="mf">3.2</span><span class="p">,</span>  <span class="mf">2.9</span><span class="p">,</span>  <span class="mf">7.2</span><span class="p">,</span>  <span class="mf">5.7</span><span class="p">,</span>  <span class="mf">5.</span> <span class="p">,</span>  <span class="mf">5.8</span><span class="p">])</span>

</code></pre></div></div> <p>Again, most of the actions are centered around zero (which makes sense, since it corresponds to the quadruped initial position, which is usually chosen to be stable), and there are almost no actions outside \([-5, 5]\) (less than 0.1%): PPO uses less than 5% of the action space!</p> <p>Now that we know that we need less than 5% of the action space to solve the task, let’s see why this might explain why SAC doesn’t work in this case<d-footnote>Action spaces that are too small are also problematic. See <a href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html">SB3 RL Tips and Tricks</a>.</d-footnote>.</p> <h2 id="sac-squashed-gaussian">SAC Squashed Gaussian</h2> <p>SAC and other off-policy algorithms for continuous actions (such as DDPG, TD3, or TQC) have an additional transformation at the end of the actor network. In SAC, actions are sampled from an unbounded Gaussian distribution and then passed through a <a href="https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html">\(tanh()\)</a> function to squash them to the range \([-1, 1]\). SAC then linearly rescales the sampled action to match the action space definition, i.e. it transforms the action from \([-1, 1]\) to \([\text{low}, \text{high}]\)<d-footnote>Rescale from [-1, 1] to [low, high] using <code>action = low + (0.5 * (scaled_action + 1.0) * (high - low))</code>.</d-footnote>.</p> <p>What does this mean? Assuming we start with a standard deviation similar to PPO, this is what the sampled action distribution looks like after squashing<d-footnote>Common PPO implementations clip the actions to fit the desired boundaries, which has the effect of oversampling actions at the boundaries when the limits are smaller than ~4.</d-footnote>:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-sac-massive-sim/squashed_vs_gaussian.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-sac-massive-sim/squashed_vs_gaussian.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> The equivalent initial squashed Gaussian distribution. </div> <p>And after rescaling to the environment limits (with PPO distribution to put it in perspective):</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-sac-massive-sim/squashed_rescaled.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-sac-massive-sim/squashed_rescaled.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> The same initial squashed Gaussian distribution but rescaled to the Unitree A1 action space $$[-100, 100]$$ </div> <p>As you can see, these are two completely different initial distributions at the beginning of training! The fact that the actions are rescaled to fit the action space boundaries explains the very large movements seen during training. Also, it explains why it was impossible for SAC to learn anything useful.</p> <h2 id="quick-fix">Quick Fix</h2> <p>When I discovered that the action limits were way too large, my first reflex was to re-train SAC, but with only 3% of the action space, to more or less match the effective action space of PPO. Although it didn’t reach PPO performance, there was finally some sign of life (an average episodic return slightly positive after a while).</p> <p>Next, I tried to use a neural network similar to the one used by PPO for this task and reduce SAC exploration by having a smaller entropy coefficient<d-footnote>The entropy coeff is the coeff that does the trade-off between RL objective and entropy maximization.</d-footnote> at the beginning of training. Bingo! SAC finally learned to solve the task!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-sac-massive-sim/learning_curve.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-sac-massive-sim/learning_curve.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Learning curve on the Unitree A1 task using 1024 envs. </div> <figure> <video src="https://b2drop.eudat.eu/public.php/dav/files/z5LFrzLNfrPMd9o/sac_trained_cut_1.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> <div class="caption"> Trained SAC agent after the quick fix. </div> <p>SAC Hyperparameters (the ones not specified are <a href="https://github.com/araffin/sbx/blob/8238fccc19048340870e4869813835b8fb02e577/sbx/sac/sac.py#L54-L64">SB3 defaults</a>):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sac_hyperparams</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">(</span>
    <span class="n">policy_kwargs</span><span class="o">=</span><span class="p">{</span>
        <span class="c1"># Similar to PPO network tuned for Unitree A1 task
</span>        <span class="sh">"</span><span class="s">activation_fn</span><span class="sh">"</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">elu</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">net_arch</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span>
    <span class="p">},</span>
    <span class="c1"># When using 2048 envs, gradient_steps=512 corresponds
</span>    <span class="c1"># to an update-to-data ratio of 1/4
</span>    <span class="n">gradient_steps</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">ent_coef</span><span class="o">=</span><span class="sh">"</span><span class="s">auto_0.006</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div> <h2 id="transition-what-does-that-mean-for-the-rl-community">Transition: What Does That Mean for the RL Community?</h2> <p>When I discovered the large action limits problem, I was curious to see how widespread it was in the community. After a quick search, it turns out that a lot of papers/code are affected<d-footnote>A notable exception are Brax-based environments because their PPO implementation uses a squashed Gaussian, so the boundaries of the environments had to be properly defined.</d-footnote> by this large boundary problem (see a non-exhaustive <a href="#appendix-affected-paperscode">list of affected papers/code below</a>).</p> <p>Although the initial choice of bounds may be a conscious and convenient one (no need to specify the real bounds, PPO will figure it out), it seems to have worked a bit by accident for those who built on top of it, and probably discouraged practitioners from trying other algorithms.</p> <p>My recommendation would be to always have properly defined action bounds. If they are not known in advance, you can <a href="#appendix-plot-action-distribution">plot the action distribution</a> and adjust the limits when iterating on the environment design <d-footnote>More on that very soon ;)</d-footnote>.</p> <h2 id="tuning-for-speed-part-ii">Tuning for Speed (Part II)</h2> <p>Although SAC can now solve the locomotion task on flat ground, it takes more time to train, is not consistent, and the performance is slightly below PPO’s. In addition, SAC’s learned gaits are not as pleasing as PPO’s, for example, SAC agents tend to keep one leg up in the air…</p> <!--[Part II](../tune-sac-isaac-sim/) explores these aspects (and more environments), reviews SAC design decisions (for example, try to remove the squashed Gaussian), and tunes it for speed, but for now, let's see what this means for the RL community.--> <p>The second part of this post explores these aspects<d-footnote>I also present the ideas that didn't work and could use help (open problems) at the end of this post.</d-footnote>, as well as more complex environments. It also details how to automatically tune SAC for speed (i.e., minimize wall clock time), to learn as fast as PPO.</p> <h2 id="defining-proper-action-bound---extracting-the-limits-with-ppo">Defining Proper Action Bound - Extracting the Limits with PPO</h2> <p>First, let’s define the action space more precisely. Correctly defining the boundaries of the action space is important for both the convergence speed and the final performance. A larger action space gives the agent more flexibility, which can lead to better performance, but slower learning. Conversely, a smaller action space can accelerate learning, though it may result in suboptimal solutions.</p> <p>Thus, rather than simply restricting the action space to a small percentage of the original, I <a href="#appendix-plot-action-distribution">recorded</a> the actions taken by a trained PPO agent and took the 2.5th and 97.5th percentiles for the new limits. In other words, the new action space contains 95% of the actions commanded by a trained PPO agent<d-footnote>I repeat the same process for any new environment where those boundaries would not work</d-footnote>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># np.percentile(actions, 2.5, axis=0)
</span><span class="n">low</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7</span><span class="p">])</span>
<span class="c1"># np.percentile(actions, 97.5, axis=0)
</span><span class="n">high</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">3.8</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">])</span>
</code></pre></div></div> <h2 id="need-for-speed-or-how-i-learned-to-stop-worrying-about-sample-efficiency">Need for Speed or: How I Learned to Stop Worrying About Sample Efficiency</h2> <p>The second aspect I can improve is the hyperparameters of the SAC algorithm. The default hyperparameters of the SAC algorithm are optimized for sample efficiency. While this is ideal for learning directly on a single real robot<d-cite key="haarnoja2018learning"></d-cite>, it is suboptimal for training thousands of robots in simulation.</p> <p><a href="#quick-fix">Previously</a>, I quickly tuned SAC by hand to get it up and running. This was sufficient for obtaining initial results, but it would be very time-consuming to continue tuning manually to reach PPO’s performance level. That’s why I turned to automatic hyperparameter <a href="https://github.com/optuna/optuna">optimization</a>.</p> <h3 id="new-objective-learn-as-fast-as-possible">New Objective: Learn as Fast as Possible</h3> <p>Since I’m using a massively parallel simulator, I no longer care about how many samples are needed to learn something but how quickly it can learn, regardless of the number of samples used. In practice, this translates to an objective function that looks like this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">objective</span><span class="p">(</span><span class="n">trial</span><span class="p">:</span> <span class="n">optuna</span><span class="p">.</span><span class="n">Trial</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Optimize for best performance after 5 minutes of training.</span><span class="sh">"""</span>
    <span class="c1"># Sample hyperparameters
</span>    <span class="n">hyperparams</span> <span class="o">=</span> <span class="nf">sample_sac_params</span><span class="p">(</span><span class="n">trial</span><span class="p">)</span>
    <span class="n">agent</span> <span class="o">=</span> <span class="n">sbx</span><span class="p">.</span><span class="nc">SAC</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span> <span class="o">**</span><span class="n">hyperparams</span><span class="p">)</span>
    <span class="c1"># Callback to exit the training loop after 5 minutes
</span>    <span class="n">callback</span> <span class="o">=</span> <span class="nc">TimeoutCallback</span><span class="p">(</span><span class="n">timeout</span><span class="o">=</span><span class="mi">60</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
    <span class="c1"># Train with a max budget of 50_000_000 timesteps
</span>    <span class="n">agent</span><span class="p">.</span><span class="nf">learn</span><span class="p">(</span><span class="n">total_timesteps</span><span class="o">=</span><span class="nf">int</span><span class="p">(</span><span class="mf">5e7</span><span class="p">),</span> <span class="n">callback</span><span class="o">=</span><span class="n">callback</span><span class="p">)</span>
    <span class="c1"># Log the number of interactions with the environments
</span>    <span class="n">trial</span><span class="p">.</span><span class="nf">set_user_attr</span><span class="p">(</span><span class="sh">"</span><span class="s">num_timesteps</span><span class="sh">"</span><span class="p">,</span> <span class="n">agent</span><span class="p">.</span><span class="n">num_timesteps</span><span class="p">)</span>
    <span class="c1"># Evaluate the trained agent
</span>    <span class="n">env</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">args_cli</span><span class="p">.</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">mean_reward</span><span class="p">,</span> <span class="n">std_reward</span> <span class="o">=</span> <span class="nf">evaluate_policy</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">n_eval_episodes</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mean_reward</span>
</code></pre></div></div> <p>The agent is evaluated after five minutes of training, regardless of how many interactions with the environment were needed (the <code class="language-plaintext highlighter-rouge">TimeoutCallback</code> forces the agent to exit the training loop).</p> <h3 id="sac-hyperparameters-sampler">SAC Hyperparameters Sampler</h3> <p>Similar to PPO, many hyperparameters can be tuned for SAC. After some trial and error, I came up with the following sampling function (I’ve included comments that explain the meaning of each parameter):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sample_sac_params</span><span class="p">(</span><span class="n">trial</span><span class="p">:</span> <span class="n">optuna</span><span class="p">.</span><span class="n">Trial</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
    <span class="c1"># Discount factor
</span>    <span class="n">gamma</span> <span class="o">=</span> <span class="n">trial</span><span class="p">.</span><span class="nf">suggest_float</span><span class="p">(</span><span class="sh">"</span><span class="s">gamma</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.975</span><span class="p">,</span> <span class="mf">0.995</span><span class="p">)</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">trial</span><span class="p">.</span><span class="nf">suggest_float</span><span class="p">(</span><span class="sh">"</span><span class="s">learning_rate</span><span class="sh">"</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="mf">0.002</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="c1"># Initial exploration rate (entropy coefficient in the SAC loss)
</span>    <span class="n">ent_coef_init</span> <span class="o">=</span> <span class="n">trial</span><span class="p">.</span><span class="nf">suggest_float</span><span class="p">(</span><span class="sh">"</span><span class="s">ent_coef_init</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="c1"># From 2^7=128 to 2^12 = 4096, the mini-batch size
</span>    <span class="n">batch_size_pow</span> <span class="o">=</span> <span class="n">trial</span><span class="p">.</span><span class="nf">suggest_int</span><span class="p">(</span><span class="sh">"</span><span class="s">batch_size_pow</span><span class="sh">"</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="c1"># How big should should the actor and critic networks be
</span>    <span class="c1"># net_arch = trial.suggest_categorical("net_arch", ["default", "simba", "large"])
</span>    <span class="c1"># I'm using integers to be able to use CMA-ES,
</span>    <span class="c1"># "default" is [256, 256], "large" is [512, 256, 128]
</span>    <span class="n">net_arch_complexity</span> <span class="o">=</span> <span class="n">trial</span><span class="p">.</span><span class="nf">suggest_int</span><span class="p">(</span><span class="sh">"</span><span class="s">net_arch_complexity</span><span class="sh">"</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="c1"># From 1 to 8 (how often should we update the networks, every train_freq steps in the env)
</span>    <span class="n">train_freq_pow</span> <span class="o">=</span> <span class="n">trial</span><span class="p">.</span><span class="nf">suggest_int</span><span class="p">(</span><span class="sh">"</span><span class="s">train_freq_pow</span><span class="sh">"</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="c1"># From 1 to 1024 (how many gradient steps by step in the environment)
</span>    <span class="n">gradient_steps_pow</span> <span class="o">=</span> <span class="n">trial</span><span class="p">.</span><span class="nf">suggest_int</span><span class="p">(</span><span class="sh">"</span><span class="s">gradient_steps_pow</span><span class="sh">"</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="c1"># From 1 to 32 (the policy delay parameter, similar to TD3 update)
</span>    <span class="n">policy_delay_pow</span> <span class="o">=</span> <span class="n">trial</span><span class="p">.</span><span class="nf">suggest_int</span><span class="p">(</span><span class="sh">"</span><span class="s">policy_delay_pow</span><span class="sh">"</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="c1"># Polyak coeff (soft update of the target network)
</span>    <span class="n">tau</span> <span class="o">=</span> <span class="n">trial</span><span class="p">.</span><span class="nf">suggest_float</span><span class="p">(</span><span class="sh">"</span><span class="s">tau</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c1"># Display true values
</span>    <span class="n">trial</span><span class="p">.</span><span class="nf">set_user_attr</span><span class="p">(</span><span class="sh">"</span><span class="s">batch_size</span><span class="sh">"</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="n">batch_size_pow</span><span class="p">)</span>
    <span class="n">trial</span><span class="p">.</span><span class="nf">set_user_attr</span><span class="p">(</span><span class="sh">"</span><span class="s">gradient_steps</span><span class="sh">"</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="n">gradient_steps_pow</span><span class="p">)</span>
    <span class="n">trial</span><span class="p">.</span><span class="nf">set_user_attr</span><span class="p">(</span><span class="sh">"</span><span class="s">policy_delay</span><span class="sh">"</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="n">policy_delay_pow</span><span class="p">)</span>
    <span class="n">trial</span><span class="p">.</span><span class="nf">set_user_attr</span><span class="p">(</span><span class="sh">"</span><span class="s">train_freq</span><span class="sh">"</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="n">train_freq_pow</span><span class="p">)</span>
    <span class="c1"># Note: to_hyperparams() does the convertions between sampled value and expected value
</span>    <span class="c1"># Ex: converts batch_size_pow to batch_size
</span>    <span class="c1"># This is useful when replaying trials
</span>    <span class="k">return</span> <span class="nf">to_hyperparams</span><span class="p">({</span>
        <span class="sh">"</span><span class="s">train_freq_pow</span><span class="sh">"</span><span class="p">:</span> <span class="n">train_freq_pow</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">gradient_steps_pow</span><span class="sh">"</span><span class="p">:</span> <span class="n">gradient_steps_pow</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">batch_size_pow</span><span class="sh">"</span><span class="p">:</span> <span class="n">batch_size_pow</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">tau</span><span class="sh">"</span><span class="p">:</span> <span class="n">tau</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">gamma</span><span class="sh">"</span><span class="p">:</span> <span class="n">gamma</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">learning_rate</span><span class="sh">"</span><span class="p">:</span> <span class="n">learning_rate</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">policy_delay_pow</span><span class="sh">"</span><span class="p">:</span> <span class="n">policy_delay_pow</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">ent_coef_init</span><span class="sh">"</span><span class="p">:</span> <span class="n">ent_coef_init</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">net_arch_complexity</span><span class="sh">"</span><span class="p">:</span> <span class="n">net_arch_complexity</span><span class="p">,</span>
    <span class="p">})</span>
</code></pre></div></div> <h3 id="replay-ratio">Replay Ratio</h3> <p>A metric that will be useful to understand the tuned hyperparameters is the replay ratio. The replay ratio (also known as update-to-data ratio or UTD ratio) measures the number of gradient updates performed per environment interaction or experience collected. This ratio represents how often an agent updates its parameters relative to how much new experience it gathers. For SAC, it is defined as <code class="language-plaintext highlighter-rouge">replay_ratio = gradient_steps / (num_envs * train_freq)</code>.</p> <p>In a classic setting, the replay ratio is usually greater than one when optimizing for sample efficiency. That means that SAC does at least one gradient step per interaction with the environment. However, since collecting new data is cheap in the current setting, the replay ratio tends to be lower than 1/4 (one gradient step for every four steps in the environment).</p> <h3 id="optimization-result---tuned-hyperparameters">Optimization Result - Tuned Hyperparameters</h3> <p>To optimize the hyperparameters, I used Optuna’s CMA-ES sampler<d-cite key="takuya2019optuna"></d-cite> for 100 trials<d-footnote>Here, I only optimized for the Unitree A1 flat task due to limited computation power. It would be interesting to tune SAC directly for the "Rough" variant, including `n_steps` and gSDE train frequency as hyperparameters.</d-footnote> (taking about 10 hours with a population size of 10 individuals). Afterward, I retrained the best trials to filter out any lucky seeds<d-cite key="raffin2022learning"></d-cite>, i.e., to find hyperparameters that work consistently across different runs.</p> <p>This is what the optimization history looks like. Many sets of hyperparameters were successful:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-sac-massive-sim/optuna_sac-480.webp 480w,/2026/assets/img/2026-04-27-sac-massive-sim/optuna_sac-800.webp 800w,/2026/assets/img/2026-04-27-sac-massive-sim/optuna_sac-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-sac-massive-sim/optuna_sac.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Hyperparameter optimization history </div> <p>These are the tuned hyperparameters of SAC found by the CMA-ES sampler while optimizing for speed:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">batch_size</span><span class="pi">:</span> <span class="m">512</span>
<span class="na">buffer_size</span><span class="pi">:</span> <span class="s">2_000_000</span>
<span class="na">ent_coef</span><span class="pi">:</span> <span class="s">auto_0.009471776840423638</span>
<span class="na">gamma</span><span class="pi">:</span> <span class="m">0.983100250213744</span>
<span class="na">gradient_steps</span><span class="pi">:</span> <span class="m">32</span>
<span class="na">learning_rate</span><span class="pi">:</span> <span class="m">0.00044689099625712413</span>
<span class="na">learning_starts</span><span class="pi">:</span> <span class="m">2000</span>
<span class="na">policy</span><span class="pi">:</span> <span class="s">MlpPolicy</span>
<span class="na">policy_delay</span><span class="pi">:</span> <span class="m">8</span>
<span class="na">policy_kwargs</span><span class="pi">:</span>
  <span class="na">net_arch</span><span class="pi">:</span> <span class="pi">[</span><span class="nv">512</span><span class="pi">,</span> <span class="nv">256</span><span class="pi">,</span> <span class="nv">128</span><span class="pi">]</span>
  <span class="na">activation_fn</span><span class="pi">:</span> <span class="kt">!!python/name:isaaclab_rl.sb3.elu</span> <span class="s1">'</span><span class="s">'</span>
  <span class="na">optimizer_class</span><span class="pi">:</span> <span class="kt">!!python/name:optax._src.alias.adamw</span> <span class="s1">'</span><span class="s">'</span>
  <span class="na">layer_norm</span><span class="pi">:</span> <span class="kc">true</span>
<span class="na">tau</span><span class="pi">:</span> <span class="m">0.0023055560568780655</span>
<span class="na">train_freq</span><span class="pi">:</span> <span class="m">1</span>
</code></pre></div></div> <p>Compared to the default hyperparameters of SAC, there are some notable changes:</p> <ul> <li>The network architecture is much larger (<code class="language-plaintext highlighter-rouge">[512, 256, 128]</code> vs. <code class="language-plaintext highlighter-rouge">[256, 256]</code>), but similar to that used by <a href="https://github.com/isaac-sim/IsaacLab/blob/f52aa9802780e897c184684d1cbc2025fafcef4a/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/a1/agents/rsl_rl_ppo_cfg.py#L21">PPO in Isaac Sim</a>.</li> <li>The lower replay ratio (RR ≈ 0.03 for 1024 environments, or three gradient steps every 100 steps in an environment) and higher policy delay (update the actor once every eight critic updates) make it faster, as less time is taken for gradient updates.</li> <li>The discount factor is lower than the default value of 0.99, which favors shorter-term rewards.</li> </ul> <p>Here is the result in video and the associated learning curves<d-footnote>The results are plotted for only five independent runs (random seeds). This is usually insufficient for RL due to the stochasticity of the results. However, in this case, the results tend to be consistent between runs (limited variability). I observed this during the many runs I did while debugging (and writing this blog post), so the trend is likely correct, even with a limited number of seeds.</d-footnote>:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-sac-massive-sim/learning_curve_unitree.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-sac-massive-sim/learning_curve_unitree.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Learning curve on the Unitree A1 task using 1024 envs. </div> <figure> <video src="https://b2drop.eudat.eu/public.php/dav/files/ATn25xMbccroaiQ/sac_unitree_a1_tuned.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> <div class="caption"> Trained SAC agent after automatic tuning. </div> <p>With these tuned hyperparameters, SAC learns faster, achieves higher performance, and the learned gaits look better (no more feet in the air!). What more could you ask for?</p> <h2 id="does-it-work---more-environments">Does it work? - More Environments</h2> <p>After it successfully learned in the flat Unitree A1 environment, I tested the same hyperparameters (with the same recipe<d-footnote>I updated the limits for each family of robots. The PPO percentiles technique worked nicely.</d-footnote>) on the GO1, GO2, Anymal-B, and Anymal-C environments, as well as the flat <a href="https://github.com/louislelay/disney_bdx_rl_isaaclab">Disney BD-X</a> environment, and … it worked!</p> <figure> <video src="https://b2drop.eudat.eu/public.php/dav/files/ATn25xMbccroaiQ/isaac_part_two.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> <div class="caption"> Trained SAC agent in different environments, using the same tuned hyperparameters. </div> <p>Then, I trained SAC on the “rough” locomotion environments, which are harder environments where the robot has to learn to navigate steps and uneven, accidented terrain (with additional randomization). And … it worked partially.</p> <h2 id="solving-harder-environments">Solving Harder Environments</h2> <h3 id="identifying-the-problem-why-it-doesnt-work">Identifying the problem: Why it doesn’t work?</h3> <p>In the “Rough” environment, the SAC-trained agent exhibits inconsistent behavior. For example, one time the robot successfully climbs down the pyramid steps without falling; at other times, however, it does nothing. Additionally, no matter how long it is trained, SAC does not seem to be able to learn to solve the “inverted pyramid”, which is probably one of the most challenging tasks:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-sac-massive-sim/inverted_pyramid-480.webp 480w,/2026/assets/img/2026-04-27-sac-massive-sim/inverted_pyramid-800.webp 800w,/2026/assets/img/2026-04-27-sac-massive-sim/inverted_pyramid-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-sac-massive-sim/inverted_pyramid.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> The inverted pyramid task. </div> <p>I isolated this task by training SAC only on the inverted pyramid. Upon further inspection, it appeared to be an exploration problem; that is, SAC never experiences successful stepping when executing random movements. This reminded me of SAC failing on the <a href="https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/">mountain car problem</a> because the exploration was inconsistent (the default high-frequency noise is usually a bad default<d-cite key="raffin2021gsde"></d-cite> for robots).</p> <h3 id="improving-exploration-and-performance">Improving Exploration and Performance</h3> <p>To test this hypothesis, I simplified the problem by <a href="https://github.com/isaac-sim/IsaacLab/blob/f52aa9802780e897c184684d1cbc2025fafcef4a/source/isaaclab/isaaclab/terrains/config/rough.py#L32">lowering the step</a> of the inverted pyramid. I also used a more consistent exploration scheme: generalized State-Dependent Exploration (gSDE)<d-cite key="raffin2021gsde"></d-cite>. </p> <p>In its simplest form, gSDE repeats the noise vector for \(n\)-steps, instead of sampling it at every timestep. In other words, instead of selecting actions following \(a_t = \mu_\theta(s_t) + \epsilon_t\)<d-footnote>$$\mu_\theta(s_t)$$ is the actor network output, which represents the mean of the Gaussian distribution.</d-footnote> and sampling \(\epsilon_t \sim N(0, \sigma^2)\) at every step during exploration, gSDE samples \(\epsilon \sim N(0, \sigma^2)\) once and keeps \(\epsilon\) constant for \(n\)-steps. The robot could finally learn to partially solve this task with this improved exploration. </p> <figure> <video src="https://b2drop.eudat.eu/public.php/dav/files/ATn25xMbccroaiQ/sac_rough_anymal_c.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> <div class="caption"> Trained SAC agent with gSDE and n-step return in the "Rough" Anymal-C environment. </div> <p>There was still a big gap in final performance between SAC and PPO. To close the gap, I drew inspiration from the recent FastTD3<d-cite key="seo2025fasttd3"></d-cite> paper and implemented n-step returns. Using <code class="language-plaintext highlighter-rouge">n_steps=3</code> allowed SAC to finally solve the hardest task<d-footnote>Although there is still a slight performance gap between SAC and PPO, after reading the FastTD3 paper and conducting my own experiments, I believe that the environment rewards were tuned for PPO to encourage a desired behavior. In other words, I suspect that the weighting of the reward terms was adjusted for PPO. To achieve similar performance, SAC probably needs different weights. However, this is beyond the scope of this already lengthy blog post.</d-footnote>!</p> <p>In summary, here are the additional manual changes I made to the hyperparameters of SAC compared to those optimized automatically:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Note: we must use train_freq &gt; 1 to enable gSDE</span>
<span class="c1"># which resamples the noise every n steps (here every 10 steps)</span>
<span class="na">train_freq</span><span class="pi">:</span> <span class="m">10</span>
<span class="c1"># Scaling the gradient steps accordingly, to keep the same replay ratio:</span>
<span class="c1"># 32 * train_freq = 320</span>
<span class="na">gradient_steps</span><span class="pi">:</span> <span class="m">320</span>
<span class="na">use_sde</span><span class="pi">:</span> <span class="s">True</span>
<span class="c1"># N-step return</span>
<span class="na">n_steps</span><span class="pi">:</span> <span class="m">3</span>
</code></pre></div></div> <p>And here are the associated learning curves (plotting the current curriculum level on the y-axis<d-footnote>I'm plotting the current state of the terrain curriculum (the higher the number, the harder the task/terrain) as the reward magnitude doesn't tell the whole story for the "Rough" task.</d-footnote>):</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-sac-massive-sim/learning_curve_rough.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-sac-massive-sim/learning_curve_rough.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Learning curve on the Anymal-C "Rough" task using 1024 envs (except for PPO). </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-sac-massive-sim/learning_curve_rough_efficiency.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-sac-massive-sim/learning_curve_rough_efficiency.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Learning curve in term of sample-effiency on the Anymal-C "Rough" task using 1024 envs (except for PPO). </div> <p>In those plots, you can see the effect of gSDE and the use of n-step returns. SAC is also much more sample-efficient than PPO.</p> <h2 id="conclusion">Conclusion</h2> <p>This concludes the long journey I started a few months ago to make SAC work on a massively parallel simulator. During this adventure, I addressed a common issue that prevents SAC-like algorithms from working in these environments: the use of an unbounded action space.</p> <p>In the end, with a proper action space and tuned hyperparameters, SAC is now competitive with PPO<d-footnote>Although there is still a slight performance gap between SAC and PPO, after reading the FastTD3 paper and conducting my own experiments, I believe that the environment rewards were tuned for PPO to encourage a desired behavior. In other words, I suspect that the weighting of the reward terms was adjusted for PPO. To achieve similar performance, SAC probably needs different weights. However, this is beyond the scope of this already lengthy blog post.</d-footnote> in terms of training time (while being much more sample-efficient) on a large collection of locomotion environments. I hope my voyage encourages others to use SAC in their experiments and unlock fine-tuning on real robots after pretraining in simulation.</p> <h2 id="appendix-affected-paperscode">Appendix: Affected Papers/Code</h2> <p>Please find here a non-exhaustive list of papers/code affected by the large bound problem: </p> <ul> <li><a href="https://github.com/isaac-sim/IsaacLab/blob/c4bec8fe01c2fd83a0a25da184494b37b3e3eb61/source/isaaclab_rl/isaaclab_rl/sb3.py#L154">IsaacLab</a></li> <li><a href="https://github.com/leggedrobotics/legged_gym/blob/17847702f90d8227cd31cce9c920aa53a739a09a/legged_gym/envs/base/legged_robot_config.py#L164">Learning to Walk in Minutes</a></li> <li><a href="https://github.com/nico-bohlinger/one_policy_to_run_them_all/blob/d9d166c348496c9665dd3ebabc20efb6d8077161/one_policy_to_run_them_all/environments/unitree_a1/environment.py#L140">One Policy to Run Them All</a></li> <li><a href="https://github.com/Argo-Robot/quadrupeds_locomotion/blob/45eec904e72ff6bafe1d5378322962003aeff88d/src/go2_train.py#L104">Genesis env</a></li> <li><a href="https://github.com/LeCAR-Lab/ASAP/blob/c78664b6d2574f62bd2287e4b54b4f8c2a0a47a5/humanoidverse/config/robot/g1/g1_29dof_anneal_23dof.yaml#L161">ASAP Humanoid</a></li> <li><a href="https://github.com/LeCAR-Lab/ABS/blob/9b95329ffb823c15dead02be620ff96938e4d0a3/training/legged_gym/legged_gym/envs/base/legged_robot_config.py#L169">Agile But Robust</a></li> <li><a href="https://github.com/Improbable-AI/rapid-locomotion-rl/blob/f5143ef940e934849c00284e34caf164d6ce7b6e/mini_gym/envs/base/legged_robot_config.py#L209">Rapid Locomotion</a></li> <li><a href="https://github.com/MarkFzp/Deep-Whole-Body-Control/blob/8159e4ed8695b2d3f62a40d2ab8d88205ac5021a/legged_gym/legged_gym/envs/widowGo1/widowGo1_config.py#L114">Deep Whole Body Control</a></li> <li><a href="https://github.com/ZiwenZhuang/parkour/blob/789e83c40b95fdd49fda7c1725c8c573df42d2a9/legged_gym/legged_gym/envs/base/legged_robot_config.py#L169">Robot Parkour Learning</a></li> </ul> <p>You can probably find many more by looking at <a href="https://scholar.google.com/scholar?cites=8503164023891275626&amp;as_sdt=2005&amp;sciodt=0,5">works that cite the ETH paper</a>.</p> <ul> <li>Seems to be fixed in <a href="https://github.com/chengxuxin/extreme-parkour/blob/d2ffe27ba59a3229fad22a9fc94c38010bb1f519/legged_gym/legged_gym/envs/base/legged_robot_config.py#L120">Extreme Parkour</a> (clip action 1.2)</li> <li>Almost fixed in <a href="https://github.com/Improbable-AI/walk-these-ways/blob/0e7236bdc81ce855cbe3d70345a7899452bdeb1c/scripts/train.py#L200">Walk this way</a> (clip action 10)</li> </ul> <h2 id="appendix-note-on-unbounded-action-spaces">Appendix: Note on Unbounded Action Spaces</h2> <p>While discussing this blog post with a fellow researcher, they raised another point that could explain why people might choose an unbounded action space.</p> <p>In short, policies can learn to produce actions outside the joint limits to trick the underlying <a href="https://en.wikipedia.org/wiki/Proportional%E2%80%93integral%E2%80%93derivative_controller">PD controller</a> into outputting desired torques. For example, when recovering from a strong push, what matters is not to accurately track a desired position, but to quickly move the joints in the right direction. This makes training almost invariant to the chosen PD gains.</p> <h2 id="appendix-what-i-tried-that-didnt-work">Appendix: What I Tried That Didn’t Work</h2> <p>While preparing this blog post, I tried many things to achieve PPO performance and learn good policies in minimal time. Many of the things I tried didn’t work, but they are probably worth investigating further. I hope you can learn from my failures, too.</p> <h3 id="using-an-unbounded-gaussian-distribution">Using an Unbounded Gaussian Distribution</h3> <p>One approach I tried was to make SAC look more like PPO. In part one, PPO could handle an unbounded action space because it used a (non-squashed) Gaussian distribution (vs. a squashed one for SAC). However, replacing SAC’s squashed Normal distribution with an unbounded Gaussian distribution led to additional problems.</p> <p>Without layer normalization in the critic, it quickly diverges (leading to Inf/NaN). It seems that, encouraged by the entropy bonus, the actor pushes toward very large action values. It also appears that this variant requires specific tuning (and that state-dependent std may need to be replaced with state-independent std, as is done for PPO).</p> <p>If you manage to reliably make SAC work with an unbounded Gaussian distribution, please reach out!</p> <h3 id="kl-divergence-adaptive-learning-rate">KL Divergence Adaptive Learning Rate</h3> <p>One component of PPO that allows for better performance is the learning rate schedule (although it is not critical, it eases hyperparameter tuning). It automatically adjusts the learning rate to maintain a constant KL divergence between two updates, ensuring that the new policy remains close to the previous one (and ensuring that the learning rate is large enough, too). It should be possible to do something similar with SAC. However, when I tried to approximate the KL divergence using either the log probability or the extracted Gaussian parameters (mean and standard deviation), it didn’t work. The KL divergence values were too large and inconsistent. SAC would probably need a trust region mechanism as well.</p> <p>Again, if you find a way to make it work, please reach out!</p> <h3 id="en-vrac---other-things-i-tried">En Vrac - Other Things I Tried</h3> <ul> <li>penalty to be away from action bounds (hard to tune)</li> <li>action space schedule (start with a small action space, make it bigger over time, tricky to schedule, and didn’t improve performance)</li> <li>linear schedule (<code class="language-plaintext highlighter-rouge">learning_rate = LinearSchedule(start=5e-4, end=1e-5, end_fraction=0.15)</code>), it helped for convergence when using <code class="language-plaintext highlighter-rouge">n_steps=1</code> and <code class="language-plaintext highlighter-rouge">use_sde=False</code>, but was not needed at the end</li> </ul> <h2 id="appendix-plot-action-distribution">Appendix: Plot Action Distribution</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="n">gymnasium</span> <span class="kn">import</span> <span class="n">spaces</span>
<span class="kn">from</span> <span class="n">stable_baselines3</span> <span class="kn">import</span> <span class="n">PPO</span>
<span class="kn">from</span> <span class="n">stable_baselines3.common.env_util</span> <span class="kn">import</span> <span class="n">make_vec_env</span>
<span class="kn">from</span> <span class="n">stable_baselines3.common.vec_env</span> <span class="kn">import</span> <span class="n">VecEnvWrapper</span>

<span class="n">sns</span><span class="p">.</span><span class="nf">set_theme</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">PlotActionVecEnvWrapper</span><span class="p">(</span><span class="n">VecEnvWrapper</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    VecEnv wrapper for plotting the taken actions.
    </span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">venv</span><span class="p">,</span> <span class="n">plot_freq</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">(</span><span class="n">venv</span><span class="p">)</span>
        <span class="c1"># Action buffer
</span>        <span class="k">assert</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">action_space</span><span class="p">,</span> <span class="n">spaces</span><span class="p">.</span><span class="n">Box</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_actions</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">actions</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">plot_freq</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_envs</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_actions</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_steps</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">plot_freq</span> <span class="o">=</span> <span class="n">plot_freq</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">venv</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">step_wait</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">obs</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">dones</span><span class="p">,</span> <span class="n">infos</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">venv</span><span class="p">.</span><span class="nf">step_wait</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">obs</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">dones</span><span class="p">,</span> <span class="n">infos</span>

    <span class="k">def</span> <span class="nf">step_async</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">actions</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">actions</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">n_steps</span> <span class="o">%</span> <span class="n">self</span><span class="p">.</span><span class="n">plot_freq</span><span class="p">]</span> <span class="o">=</span> <span class="n">actions</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_steps</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">n_steps</span> <span class="o">%</span> <span class="n">self</span><span class="p">.</span><span class="n">plot_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">plot</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">venv</span><span class="p">.</span><span class="nf">step_async</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="c1"># Flatten the env dimension
</span>        <span class="n">actions</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">actions</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_actions</span><span class="p">)</span>
        <span class="n">n_steps</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">num_envs</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">n_steps</span>
        <span class="c1"># Create a figure with subplots for each action dimension
</span>        <span class="n">n_rows</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_actions</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">n_cols</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_actions</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">n_rows</span><span class="p">,</span> <span class="n">n_cols</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
        <span class="n">fig</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span>
            <span class="sa">f</span><span class="sh">"</span><span class="s">Distribution of Actions per Dimension after </span><span class="si">{</span><span class="n">n_steps</span><span class="si">}</span><span class="s"> steps</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span>
        <span class="p">)</span>

        <span class="c1"># Flatten the axes array for easy iteration
</span>        <span class="k">if</span> <span class="n">n_rows</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Special case, n_actions == 1
</span>            <span class="n">axes</span> <span class="o">=</span> <span class="p">[</span><span class="n">axes</span><span class="p">]</span>

        <span class="c1"># Plot the distribution for each action dimension
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_actions</span><span class="p">):</span>
            <span class="n">sns</span><span class="p">.</span><span class="nf">histplot</span><span class="p">(</span><span class="n">actions</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">stat</span><span class="o">=</span><span class="sh">"</span><span class="s">density</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Action Dimension </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Action Value</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Density</span><span class="sh">"</span><span class="p">)</span>

        <span class="c1"># Adjust the layout and display the plot
</span>        <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>


<span class="n">vec_env</span> <span class="o">=</span> <span class="nf">make_vec_env</span><span class="p">(</span><span class="sh">"</span><span class="s">Pendulum-v1</span><span class="sh">"</span><span class="p">,</span> <span class="n">n_envs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">wrapped_env</span> <span class="o">=</span> <span class="nc">PlotActionVecEnvWrapper</span><span class="p">(</span><span class="n">vec_env</span><span class="p">,</span> <span class="n">plot_freq</span><span class="o">=</span><span class="mi">5_000</span><span class="p">)</span>

<span class="c1"># from sbx import PPO
# from sbx import SAC
# policy_kwargs = dict(log_std_init=-0.5)
</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">PPO</span><span class="p">(</span><span class="sh">"</span><span class="s">MlpPolicy</span><span class="sh">"</span><span class="p">,</span> <span class="n">wrapped_env</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">learn</span><span class="p">(</span><span class="n">total_timesteps</span><span class="o">=</span><span class="mi">1_000_000</span><span class="p">)</span>
</code></pre></div></div> ]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[This post details how to get the Soft-Actor Critic (SAC) and other off-policy reinforcement learning algorithms to work on massively parallel simulators (e.g., Isaac Sim with thousands of robots simulated in parallel). In addition to tuning SAC for speed, the post also explores why SAC fails where PPO succeeds, highlighting a common problem in task design that many codebases share.]]></summary></entry><entry><title type="html">The Thermodynamic Implications of GPU Cooling Systems on Transformer Model Training Efficiency During Leap Years</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/thermodynamic-gpu-cooling-leap-years/" rel="alternate" type="text/html" title="The Thermodynamic Implications of GPU Cooling Systems on Transformer Model Training Efficiency During Leap Years"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/thermodynamic-gpu-cooling-leap-years</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/thermodynamic-gpu-cooling-leap-years/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>The field of machine learning has long overlooked a critical environmental factor that significantly impacts neural network training: the thermodynamic irregularities introduced by leap years. While researchers have extensively studied the effects of learning rates, batch sizes, and architectural choices, the subtle but profound impact of the additional 24-hour period in leap years on GPU thermal dynamics remains largely unexplored.</p> <p>This work presents the first comprehensive analysis of how leap year calendar anomalies affect the thermal behavior of GPU cooling systems, with cascading effects on transformer model convergence patterns. Our findings suggest that the Earth’s orbital mechanics, specifically the 0.25-day adjustment required every four years, creates measurable perturbations in datacenter thermal equilibrium that correlate with attention mechanism efficiency.</p> <h2 id="methodology">Methodology</h2> <h3 id="leap-year-detection-algorithms">Leap Year Detection Algorithms</h3> <p>We developed a novel leap year detection system integrated directly into the PyTorch training loop:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">is_leap_year_aware_forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">current_date</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">current_date</span><span class="p">.</span><span class="n">year</span> <span class="o">%</span> <span class="mi">4</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># Apply leap year thermal correction factor
</span>        <span class="n">thermal_adjustment</span> <span class="o">=</span> <span class="mf">0.000001</span> <span class="o">*</span> <span class="p">(</span><span class="n">current_date</span><span class="p">.</span><span class="nf">timetuple</span><span class="p">().</span><span class="n">tm_yday</span> <span class="o">/</span> <span class="mi">366</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">thermal_adjustment</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">transformer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <h3 id="thermal-monitoring-protocols">Thermal Monitoring Protocols</h3> <p>Our experimental setup consisted of 42 identical A100 GPUs arranged in a hexagonal formation (chosen for optimal heat distribution symmetry). Temperature sensors were placed at precisely 17.3cm intervals around each GPU, measuring thermal fluctuations at 0.1-second intervals throughout the entire leap year period of 2024.</p> <p><em>Note: Hexagonal GPU arrangement diagram would be shown here in the final publication. The sacred geometry ensures minimal interference from non-leap-year thermal patterns.</em></p> <h2 id="results">Results</h2> <h3 id="temperature-variance-analysis">Temperature Variance Analysis</h3> <p>Our results demonstrate a clear 0.003°C temperature increase in GPU cores during leap years, with peak thermal anomalies occurring on February 29th at exactly 11:47 AM GMT (±3 minutes). This temperature spike correlates with a measurable 0.0001% decrease in floating-point precision during matrix multiplications.</p> \[T_{leap} = T_{normal} + \alpha \cdot \sin(\frac{2\pi \cdot d}{366}) \cdot \beta\] <p>Where:</p> <ul> <li>$T_{leap}$ is the GPU temperature during leap years</li> <li>$\alpha = 0.003°C$ (leap year thermal coefficient)</li> <li>$d$ is the day number within the leap year</li> <li>$\beta$ is the transformer attention head correction factor</li> </ul> <h3 id="attention-head-performance-metrics">Attention Head Performance Metrics</h3> <p>Most remarkably, we observed that transformer models trained during leap years exhibit superior performance on temporal reasoning tasks, with a 0.0002% improvement in accuracy when processing time-series data. This suggests that the additional day creates a harmonic resonance in the attention mechanisms that enhances temporal pattern recognition.</p> <p><em>Figure 2: Attention weight visualization during leap year training would be displayed here. Note the subtle but distinct pattern emergence on day 366, visible only under electron microscopy.</em></p> <h2 id="discussion">Discussion</h2> <p>The implications of our findings extend far beyond simple thermal management. The correlation between leap year calendar mechanics and neural network performance suggests a deeper connection between astronomical phenomena and computational systems. We hypothesize that the Earth’s orbital adjustment creates subtle gravitational perturbations that affect electron flow in GPU transistors, leading to the observed thermal variations.</p> <p>Furthermore, our work raises important questions about the temporal bias in machine learning datasets. Should we adjust learning rates based on whether training occurs during a leap year? Should model architectures include leap year awareness as a fundamental component?</p> <p>We recommend that all future ML papers include a “Leap Year Disclosure Statement” specifying whether experiments were conducted during leap years, similar to current reproducibility standards.</p> <h2 id="conclusion">Conclusion</h2> <p>This groundbreaking research demonstrates that leap years have measurable, statistically significant effects on GPU thermal dynamics and transformer model training efficiency. The 0.003°C temperature variance and corresponding 0.0002% accuracy improvement may seem negligible, but in the context of large-scale neural network training, these effects compound to produce meaningful differences in model performance.</p> <p>Our work opens new avenues for “calendar-aware computing” and suggests that future AI systems should incorporate astronomical and calendrical awareness into their training protocols. We call upon the research community to establish standardized leap year correction factors for all major deep learning frameworks.</p> <p>As we stand on the precipice of AGI, it is crucial that we consider all environmental factors—including the Earth’s orbital mechanics—that may influence our artificial neural networks. Only by acknowledging the deep connection between celestial mechanics and silicon-based computation can we truly achieve calendar-invariant artificial intelligence.</p> <hr/> <p><em>This research was conducted with the highest standards of scientific rigor and was peer-reviewed by the International Committee for Temporal Computing Standards. All experiments were performed in compliance with the Geneva Convention on GPU Thermal Ethics.</em></p>]]></content><author><name>Dr. Thermal McProcessorface</name></author><summary type="html"><![CDATA[A comprehensive analysis of how the additional day in leap years affects thermal dissipation patterns in datacenter GPU arrays, with particular emphasis on the correlation between Gregorian calendar anomalies and attention mechanism convergence rates.]]></summary></entry></feed>